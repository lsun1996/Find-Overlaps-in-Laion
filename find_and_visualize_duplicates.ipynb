{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torchvision import transforms\n",
    "import json\n",
    "\"\"\"\n",
    "Parameters:\n",
    "\n",
    "- datset_name: also include train/test split\n",
    "\n",
    "- hf_dataset: the dataset identifier in huggingface;\n",
    "\n",
    "- optimized_dir: directory to save optimized_dataset;\n",
    "\n",
    "- image_key: \"webp\", \"jpg\" or \"png\";\n",
    "\n",
    "- id_key: the id attribute in dataset card, default is \"__key__\"\n",
    "\n",
    "- label_key: the text/caption attribute in dataset card, default is \"cls\" \n",
    "\n",
    "- threshold(inclusive): the maximum distance for search results, in hamming distance, how many bits in the hash string is different.\n",
    "\n",
    "- perceptual hash or average hash\n",
    "\n",
    "- k: the number of results to show\n",
    "\n",
    "NOTE: need to provicde classes as a list if labels are numerical ELSE LEFT THE CLASSES LIST EMPTY\n",
    "\"\"\"\n",
    "\n",
    "dataset_name = \"cub\"\n",
    "split = \"test\"\n",
    "\n",
    "threshold = 5\n",
    "method = \"perceptual\" \n",
    "\n",
    "k = 5\n",
    "\n",
    "\"\"\"LEFT CLASSES LIST EMPTY IF NO NEED TO PROVIDE IT\"\"\"\n",
    "classes = json.load(open(f\"data/classes_{dataset_name}.json\", \"r\"))\n",
    "print(len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name == \"cifar100\":\n",
    "    hf_dataset = load_dataset(\"clip-benchmark/wds_vtab-cifar100\", split=split, streaming=False)\n",
    "elif dataset_name == \"caltech101\":\n",
    "    hf_dataset = load_dataset(\"clip-benchmark/wds_vtab-caltech101\", split=split, streaming=False)\n",
    "elif dataset_name == \"food101\":\n",
    "    hf_dataset = load_dataset(\"clip-benchmark/wds_food101\", split=split, streaming=False)\n",
    "elif dataset_name == \"cars\":\n",
    "    hf_dataset = load_dataset(\"clip-benchmark/wds_cars\", split=split, streaming=False)\n",
    "elif dataset_name == \"country211\":\n",
    "    hf_dataset = load_dataset(\"clip-benchmark/wds_country211\", split=split, streaming=False)\n",
    "elif dataset_name == \"sun397\":\n",
    "    hf_dataset = load_dataset(\"clip-benchmark/wds_sun397\", split=split, streaming=False)\n",
    "elif dataset_name == \"fer2013\":\n",
    "    hf_dataset = load_dataset(\"clip-benchmark/wds_fer2013\", split=split, streaming=False)\n",
    "elif dataset_name == \"aircraft\":\n",
    "    hf_dataset = load_dataset(\"clip-benchmark/wds_fgvc_aircraft\", split=split, streaming=False)\n",
    "elif dataset_name == \"imagenetv2\":\n",
    "    hf_dataset = load_dataset(\"clip-benchmark/wds_imagenetv2\", split=split, streaming=False)\n",
    "elif dataset_name == \"imagenet-o\":\n",
    "    hf_dataset = load_dataset(\"clip-benchmark/wds_imagenet-o\", split=split, streaming=False)\n",
    "elif dataset_name == \"pets\":\n",
    "    hf_dataset = load_dataset(\"clip-benchmark/wds_vtab-pets\", split=split, streaming=False)\n",
    "elif dataset_name == \"imagenet-a\":\n",
    "    hf_dataset = load_dataset(\"clip-benchmark/wds_imagenet-a\", split=split, streaming=False)\n",
    "elif dataset_name == \"imagenet-r\":\n",
    "    hf_dataset = load_dataset(\"clip-benchmark/wds_imagenet-r\", split=split, streaming=False)\n",
    "elif dataset_name == \"cub\":\n",
    "    hf_dataset = load_dataset(\"lxs784/cub-200-2011-clip-benchmark\", split=split, streaming=False)\n",
    "\n",
    "if \"webp\" in hf_dataset[0] and hf_dataset[0][\"webp\"] is not None:\n",
    "    image_key = \"webp\"\n",
    "elif hf_dataset[0][\"jpg\"] is not None:\n",
    "    image_key = \"jpg\"\n",
    "    \n",
    "dataset_name += \"-\" + split\n",
    "print(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset from Huggingface\n",
    "\n",
    "Optimize hf dataset for fast search and retrival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "def optimize_hf_to_lightning(hf_dataset, output_dir, image_key=\"webp\", id_key=\"__key__\", label_key=\"cls\"):\n",
    "    \"\"\"\n",
    "    Iterates over the Hugging Face dataset and saves each sample to disk in a format\n",
    "    that Lightning's StreamingDataset can read. An index file (index.json) is created.\n",
    "    \n",
    "    Each sample is stored as:\n",
    "      - An image file in JPEG format\n",
    "      - A metadata entry in the index that records the file path and label\n",
    "    \n",
    "    Parameters:\n",
    "      hf_dataset: The Hugging Face dataset (can be streaming or in-memory)\n",
    "      output_dir: Directory where the optimized dataset will be stored.\n",
    "      image_key: Field name in the dataset containing image data.\n",
    "      id_key: Field name to use as a unique identifier.\n",
    "      label_key: Field name containing label or class information.\n",
    "    Returns:\n",
    "      The output directory path (which contains the data and index).\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    index = {}\n",
    "    # serializer = JPEGSerializer()  # Can be used to serialize images if desired.\n",
    "    \n",
    "    # Iterate over the dataset and write each sample.\n",
    "    for sample in tqdm(hf_dataset):\n",
    "        uid = sample[id_key]\n",
    "        # Define a file path for the image.\n",
    "        image_filename = f\"{uid}.png\"\n",
    "        image_path = os.path.join(output_dir, image_filename)\n",
    "        \n",
    "        # Get the image. Depending on your dataset, it might already be a PIL Image.\n",
    "        image = sample[image_key]\n",
    "        if not isinstance(image, Image.Image):\n",
    "            # If image is not a PIL image, try converting it.\n",
    "            image = Image.fromarray(image)\n",
    "            \n",
    "        if image.mode != \"RGB\":\n",
    "            image = image.convert(\"RGB\")\n",
    "        # Save the image in JPEG format.\n",
    "        image.save(image_path, format=\"PNG\")\n",
    "        \n",
    "        # Record metadata in the index.\n",
    "        index[uid] = {\n",
    "            \"image_path\": image_filename,  # Store relative path\n",
    "            \"label\": sample[label_key],\n",
    "        }\n",
    "    \n",
    "    # Write out the index file.\n",
    "    index_path = os.path.join(output_dir, \"index.json\")\n",
    "    with open(index_path, \"w\") as f:\n",
    "        json.dump(index, f)\n",
    "    \n",
    "    return output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "from PIL import Image\n",
    "import imagehash\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class HFDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, index_file, lookup=None, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        with open(os.path.join(root_dir, index_file), \"r\") as f:\n",
    "            self.index_data = json.load(f)\n",
    "        self.lookup = lookup\n",
    "        self.samples = list(self.index_data.items())\n",
    "        self.uid_to_sample = dict(self.samples)\n",
    "        self.transform = transform \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        uid, sample = self.samples[index]\n",
    "        image_path = os.path.join(self.root_dir, sample[\"image_path\"])\n",
    "        pil_image = Image.open(image_path).convert(\"RGB\")\n",
    "        text = self.lookup[sample[\"label\"]] if self.lookup else sample[\"label\"]\n",
    "\n",
    "        ahash = str(imagehash.average_hash(pil_image))\n",
    "        phash = str(imagehash.phash(pil_image))\n",
    "\n",
    "        return index, text, ahash, phash, uid\n",
    "\n",
    "    def get_by_id(self, uid):\n",
    "        \"\"\"\n",
    "        Retrieve a raw PIL image and metadata by its unique identifier.\n",
    "        \"\"\"\n",
    "        # if uid not in self.uid_to_sample:\n",
    "        #     raise KeyError(f\"UID: {uid} not found in dataset.\")\n",
    "        sample = self.uid_to_sample[uid]\n",
    "        image_path = os.path.join(self.root_dir, sample[\"image_path\"])\n",
    "        pil_image = Image.open(image_path).convert(\"RGB\")\n",
    "        text = self.lookup[sample[\"label\"]] if self.lookup else sample[\"label\"]\n",
    "        ahash = imagehash.average_hash(pil_image)\n",
    "        phash = imagehash.phash(pil_image)\n",
    "\n",
    "        return pil_image, text, ahash, phash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_dir = f\"data/optimized_dataset/{dataset_name}\"\n",
    "\n",
    "if not os.path.exists(os.path.join(optimized_dir, \"index.json\")):\n",
    "    optimize_hf_to_lightning(hf_dataset, optimized_dir, image_key=image_key)\n",
    "\n",
    "dataset = HFDataset(\n",
    "        index_file = \"index.json\",\n",
    "        root_dir=optimized_dir,\n",
    "        lookup=classes if classes else None,\n",
    "        # transform = transform\n",
    "        )\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "for _, texts, ahashes, phashes, uids in dataloader:\n",
    "    print(texts, ahashes, phashes, uids)\n",
    "    break\n",
    "sample_uid = dataset.samples[0][0]\n",
    "pil_image, text, ahash, phash = dataset.get_by_id(sample_uid)\n",
    "pil_image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find duplicate images between your dataset and laion400m\n",
    "in this step, we load the pre-computed binary index for laiion, \n",
    "\n",
    "for each image in your dataset, it will compute its hash value, and find the images in laion that has similar value.\n",
    "\n",
    "Currently, the perceptual hash is applied for hash value match, the average match will be available in the future.\n",
    "\n",
    "Parameters:\n",
    "\n",
    "- threshold(inclusive): the maximum distance for search results, in hamming distance, how many bits in the hash string is different.\n",
    "- perceptual hash or average hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# if method == \"perceptual\":\n",
    "binary_index_phash = faiss.read_index_binary(\"lightning_binary_index.bin\")\n",
    "# if method == \"average\":\n",
    "#     binary_index_ahash = faiss.read_index_binary(\"lightning_binary_index_average_hash.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Make the {uid: match_indices} pairs\n",
    "\"\"\"\n",
    "def hex_to_vector(hex_str, vector_dim=16):\n",
    "    \"\"\"\n",
    "    Convert a 16-character hex string to a 64-bit binary vector. \n",
    "    Each hex digit is converted to a 4-bit binary number.\n",
    "    Ensure the hex string is exactly 16 characters long and binary vector is exactly 64 bits long.\n",
    "    \"\"\"\n",
    "    if hex_str is None:\n",
    "        return [0] * vector_dim * 4\n",
    "\n",
    "    if len(hex_str) != vector_dim:\n",
    "        raise ValueError(f\"Hex string length ({len(hex_str)}) does not match expected dimension ({vector_dim}).\")\n",
    "    \n",
    "    vector = []\n",
    "    for digit in hex_str:\n",
    "        if digit not in \"0123456789abcdef\":\n",
    "            raise ValueError(\"Invalid hex string\")\n",
    "\n",
    "        binary_str = bin(int(digit, 16))[2:].zfill(4)\n",
    "        vector.extend([int(bit) for bit in binary_str])\n",
    "\n",
    "    if len(vector) != vector_dim * 4:\n",
    "        raise ValueError(\"Hex string did not convert to the expected number of bits\")\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "results = {}\n",
    "part = 0\n",
    "json_dir = f\"/teamspace/studios/this_studio/data/intermediate/{dataset_name}/match_indices_{threshold}\"\n",
    "os.makedirs(json_dir, exist_ok=True)\n",
    "\n",
    "for i, (_, texts, ahashes, phashes, uids) in enumerate(tqdm(dataloader, desc=f\"Finding duplicates in {dataset_name}\")):\n",
    "    query_vectors = np.array([hex_to_vector(x, 16) for x in phashes], dtype='uint8')\n",
    "    queries_packed = np.packbits(query_vectors, axis=1).reshape(len(phashes), 8)\n",
    "\n",
    "    lims, D_range, I_range = binary_index_phash.range_search(queries_packed, threshold)\n",
    "\n",
    "    for q in range(queries_packed.shape[0]):\n",
    "        start = lims[q]\n",
    "        end = lims[q + 1]\n",
    "        if start == end:\n",
    "            continue\n",
    "        match_indices = I_range[start:end].tolist()\n",
    "        if len(match_indices) > 0:\n",
    "            results[uids[q]] = match_indices\n",
    "        if len(results) == 100:\n",
    "            with open(os.path.join(json_dir, f\"results_{part}.json\"), \"w\") as f:\n",
    "                json.dump(results, f)\n",
    "                tqdm.write(f\"part {part} saved!\")\n",
    "            results = {} # reset\n",
    "            part += 1\n",
    "            \n",
    "if len(results) > 0:\n",
    "    with open(os.path.join(json_dir, f\"results_{part}.json\"), \"w\") as f:\n",
    "        json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"extra step: put all results in one json\"\"\"\n",
    "import glob\n",
    "import os\n",
    "json_files = glob.glob(os.path.join(json_dir, \"*.json\"))\n",
    "\n",
    "results = {}\n",
    "for json_file in json_files:\n",
    "    with open(json_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    results.update(data)\n",
    "\n",
    "with open(os.path.join(json_dir, \"combined_results.json\"), \"w\") as f:\n",
    "    json.dump(results, f)\n",
    "print(f\"Combined results saved to {os.path.join(json_dir, 'combined_results.json')}, total duplicate images: {len(results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the laion400m dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"load the laion400m dataset for image retrival\"\"\"\n",
    "import os\n",
    "from lightning_cloud.utils.data_connection import add_s3_connection\n",
    "from lightning.data import StreamingDataset, StreamingDataLoader\n",
    "from lightning.data.streaming.serializers import JPEGSerializer\n",
    "import torchvision.transforms.v2 as T\n",
    "from tqdm import tqdm\n",
    "import imagehash\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import concurrent\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# 1. Add the prepared dataset to your teamspace\n",
    "add_s3_connection(\"laoin-400m\")\n",
    "\n",
    "# 2. Create the streaming dataset\n",
    "class LAOINStreamingDataset(StreamingDataset):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.serializer = JPEGSerializer()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        id, image, text, _, _, _ = super().__getitem__(index)\n",
    "        \n",
    "        return Image.open(io.BytesIO(image)), text, str(id)\n",
    "\n",
    "laion = LAOINStreamingDataset(input_dir=\"/teamspace/s3_connections/laoin-400m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laion[69815173][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Results\n",
    "\n",
    "Plot images for the results collected from last step\n",
    "\n",
    "Parameter:\n",
    "- k: show top-k results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "plot original image and overlap images\n",
    "one row per plot\n",
    "\"\"\"\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import textwrap\n",
    "import json\n",
    "import imagehash\n",
    "\n",
    "def resize_image(image, target_size=(256, 256)):\n",
    "    return image.resize(target_size, Image.Resampling.LANCZOS)\n",
    "\n",
    "def show_match_results_single(dataset, results, output_dir, k=5):\n",
    "    cols = k + 2\n",
    "    for uid, match_indices in tqdm(results.items(), desc=f\"plotting duplicate images for {dataset_name}\"):\n",
    "        fig, axes = plt.subplots(1, cols, figsize=(cols * 3, 3))\n",
    "        axes[0].text(0.5, 0.5, uid, fontsize=24, ha='center', va='center')\n",
    "        axes[0].axis(\"off\")\n",
    "\n",
    "        original_image, original_text, ahash, phash= dataset.get_by_id(uid)\n",
    "        original_image_resized = resize_image(original_image)\n",
    "        axes[1].imshow(original_image_resized)\n",
    "        wrapped_caption = \"\\n\".join(textwrap.wrap(original_text, width=24))\n",
    "        axes[1].set_title(wrapped_caption)\n",
    "        axes[1].axis('off')\n",
    "\n",
    "        for j in range (k):\n",
    "            ax = axes[j + 2]\n",
    "            if j >= len(match_indices):\n",
    "                ax.imshow(np.ones((1, 1, 3)))\n",
    "            else:\n",
    "                idx = match_indices[j]\n",
    "                match_image, match_text, _ = laion[idx]\n",
    "                laion_phash = imagehash.phash(match_image)\n",
    "                p_dist = abs(phash - laion_phash)\n",
    "                ax.imshow(match_image)\n",
    "                caption_match = \"p_dist: \" + str(p_dist) + \" \" + match_text\n",
    "                wrapped_lines = textwrap.wrap(caption_match, width=24)\n",
    "                wrapped_caption_match = \"\\n\".join(wrapped_lines[:2])\n",
    "                ax.set_title(wrapped_caption_match, fontsize=8)\n",
    "            ax.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, f\"{uid}.png\"))\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = f\"/teamspace/studios/this_studio/data/intermediate/{dataset_name}/match_indices_{threshold}\"\n",
    "input_file = os.path.join(input_dir, \"combined_results.json\")\n",
    "output_dir = f\"/teamspace/studios/this_studio/data/intermediate/{dataset_name}/plots-4\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "with open(input_file, \"r\") as f:\n",
    "    results = json.load(f)\n",
    "    show_match_results_single(dataset, results, output_dir, k)\n",
    "print(\"Plotted all images: \", output_dir)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
