{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torchvision import transforms\n",
    "import json\n",
    "\"\"\"\n",
    "Parameters:\n",
    "\n",
    "- datset_name: also include train/test split\n",
    "\n",
    "- k: the number of results to show\n",
    "\n",
    "- unmatch: a dictionary storing uid and list of unmatched indices.\n",
    "\n",
    "NOTE: need to provicde classes as a list if labels are numerical ELSE LEFT THE CLASSES LIST EMPTY\n",
    "NOTE: Change the prompts in the last section!\n",
    "\"\"\"\n",
    "\n",
    "dataset_name = \"imagenet-r-test\"\n",
    "# hf_dataset = load_dataset(\"clip-benchmark/wds_vtab-pets\", split=\"train\", streaming=False)\n",
    "\n",
    "# unmatch = {\"s0000624\": [0, 4], \"s0002714\": [0], \"s0005515\": [1, 2], \"s0006134\": [0], \"s0007398\": [1,2,3], \"s0009665\": [0, 3, 4], \"s0014936\": [0, 2, 3], \"s0016537\": [0, 1, 3], \"s0016710\": [0, 1],\n",
    "#     \"s0017946\": [0], \"s0021577\": [1], \"s0022921\": [1], \"s0022976\": [0, 1, 2, 3], \"s0022987\": [1], \"s0023072\": [0, 1, 2, 3], \"s0023707\": [0], \"s0023788\": [0, 1], \"s0026905\": [0, 1], \"s0028650\": [1]\n",
    "#     }\n",
    "\n",
    "unmatch = {}\n",
    "\n",
    "threshold = 4\n",
    "k = 5\n",
    "num_workers = 4\n",
    "\n",
    "\"\"\"LEFT CLASSES LIST EMPTY IF NO NEED TO PROVIDE IT\"\"\"\n",
    "classes = json.load(open(\"data/classes_imagenet-r.json\", \"r\"))\n",
    "print(len(classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load local HF dataset and laion400m dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset from Huggingface\n",
    "\n",
    "Optimize hf dataset for fast search and retrival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "from PIL import Image\n",
    "import imagehash\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class HFDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, index_file, lookup=None, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        with open(os.path.join(root_dir, index_file), \"r\") as f:\n",
    "            self.index_data = json.load(f)\n",
    "        self.lookup = lookup\n",
    "        self.samples = list(self.index_data.items())\n",
    "        self.uid_to_sample = dict(self.samples)\n",
    "        self.transform = transform \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        uid, sample = self.samples[index]\n",
    "        image_path = os.path.join(self.root_dir, sample[\"image_path\"])\n",
    "        pil_image = Image.open(image_path).convert(\"RGB\")\n",
    "        text = self.lookup[sample[\"label\"]] if self.lookup else sample[\"label\"]\n",
    "\n",
    "        ahash = str(imagehash.average_hash(pil_image))\n",
    "        phash = str(imagehash.phash(pil_image))\n",
    "\n",
    "        return index, text, ahash, phash, uid\n",
    "\n",
    "    def get_by_id(self, uid):\n",
    "        \"\"\"\n",
    "        Retrieve a raw PIL image and metadata by its unique identifier.\n",
    "        \"\"\"\n",
    "        if uid not in self.uid_to_sample:\n",
    "            raise KeyError(f\"UID {uid} not found in dataset.\")\n",
    "        sample = self.uid_to_sample[uid]\n",
    "        image_path = os.path.join(self.root_dir, sample[\"image_path\"])\n",
    "        pil_image = Image.open(image_path).convert(\"RGB\")\n",
    "        text = self.lookup[sample[\"label\"]] if self.lookup else sample[\"label\"]\n",
    "        ahash = str(imagehash.average_hash(pil_image))\n",
    "        phash = str(imagehash.phash(pil_image))\n",
    "\n",
    "        return pil_image, text, ahash, phash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_dir = f\"data/optimized_dataset/{dataset_name}\"\n",
    "\n",
    "if not os.path.exists(os.path.join(optimized_dir, \"index.json\")):\n",
    "    optimize_hf_to_lightning(hf_dataset, optimized_dir, image_key=image_key, id_key=id_key, label_key=label_key)\n",
    "\n",
    "dataset = HFDataset(\n",
    "        index_file = \"index.json\",\n",
    "        root_dir=optimized_dir,\n",
    "        lookup=classes if classes else None,\n",
    "        # transform = transform\n",
    "        )\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "for _, texts, ahashes, phashes, uids in dataloader:\n",
    "    print(texts, ahashes, phashes, uids)\n",
    "    break\n",
    "sample_uid = dataset.samples[0][0]\n",
    "pil_image, text, ahash, phash = dataset.get_by_id(sample_uid)\n",
    "pil_image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the laion400m dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"load the laion400m dataset for image retrival\"\"\"\n",
    "import os\n",
    "from lightning_cloud.utils.data_connection import add_s3_connection\n",
    "from lightning.data import StreamingDataset, StreamingDataLoader\n",
    "from lightning.data.streaming.serializers import JPEGSerializer\n",
    "import torchvision.transforms.v2 as T\n",
    "from tqdm import tqdm\n",
    "import imagehash\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import concurrent\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# 1. Add the prepared dataset to your teamspace\n",
    "add_s3_connection(\"laoin-400m\")\n",
    "\n",
    "# 2. Create the streaming dataset\n",
    "class LAOINStreamingDataset(StreamingDataset):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.serializer = JPEGSerializer()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        id, image, text, _, _, _ = super().__getitem__(index)\n",
    "        \n",
    "        return Image.open(io.BytesIO(image)), text, str(id)\n",
    "\n",
    "laion = LAOINStreamingDataset(input_dir=\"/teamspace/s3_connections/laoin-400m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laion[100][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter out non-duplicates and produce final plots\n",
    "\n",
    "Examine the output images, \n",
    "    delete any images if there are no match\n",
    "    !! examples that doesn't match the original should be wirtten in the beginning cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"data/intermediate/{dataset_name}/plots\"\n",
    "remain_uids = [uid[:-4] for uid in os.listdir(output_dir)]\n",
    "# print(remain_uid)\n",
    "results = json.load(open(f\"data/intermediate/{dataset_name}/match_indices_{threshold}/combined_results.json\", \"r\"))\n",
    "final_results = {}\n",
    "print(dataset_name, \": before deletion, length of results:\", len(results))\n",
    "for uid, indices in results.items():\n",
    "    new_indices = indices[:5]\n",
    "    if uid not in remain_uids:\n",
    "        continue\n",
    "    if uid in unmatch:\n",
    "        keys_to_del = unmatch[uid]\n",
    "        for key in reversed(keys_to_del):\n",
    "            if key >= len(new_indices):\n",
    "                print(\"Error in pop: \", uid, key)\n",
    "                continue\n",
    "            new_indices.pop(key)\n",
    "    final_results[uid] = new_indices\n",
    "\n",
    "final_dir = f\"data/final/{dataset_name}\"\n",
    "os.makedirs(final_dir, exist_ok=True)\n",
    "json.dump(final_results, open(os.path.join(final_dir, \"final_results.json\"), \"w\"))\n",
    "print(dataset_name, \": after deletion, length of results: \", len(final_results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "take large input json \n",
    "plot multiple plots\n",
    "each plot has a max_rows_count\n",
    "\"\"\"\n",
    "\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import textwrap\n",
    "import json\n",
    "import os\n",
    "\n",
    "def resize_image(image, target_size=(256, 256)):\n",
    "    return image.resize(target_size, Image.Resampling.LANCZOS)\n",
    "\n",
    "def show_match_results_long(dataset, results, output_dir, k=5, max_rows_plot=100):\n",
    "    \"\"\"\n",
    "    results: a dictionary of uid - match_indices pairs\n",
    "    \"\"\"\n",
    "    part = 0\n",
    "    cols = k + 2\n",
    "    rows = min(len(results), max_rows_plot)\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols*3, rows*3))\n",
    "\n",
    "    row_count = 0\n",
    "    for uid, match_indices in tqdm(results.items(), desc=f\"Plotting {dataset_name}\"):\n",
    "        axes[row_count, 0].text(0.5, 0.5, f\"{row_count} - {uid}\", fontsize=16, ha=\"center\", va=\"center\")\n",
    "        axes[row_count, 0].axis('off')\n",
    "\n",
    "        ax = axes[row_count, 1]\n",
    "        original_image, original_text, _, _= dataset.get_by_id(uid)\n",
    "        original_image_resized = resize_image(original_image)\n",
    "        ax.imshow(original_image_resized)\n",
    "        wrapped_lines = textwrap.wrap(original_text, width=24)[:2]\n",
    "        wrapped_caption = \"\\n\".join(wrapped_lines)\n",
    "        ax.set_title(wrapped_caption)\n",
    "        ax.axis('off')\n",
    "        for j in range (k):\n",
    "            ax = axes[row_count, j + 2]\n",
    "            if j >= len(match_indices):\n",
    "                ax.imshow(np.ones((1, 1, 3)))\n",
    "            else:\n",
    "                idx = match_indices[j]\n",
    "                match_image, match_text, _ = laion[idx]\n",
    "                ax.imshow(match_image)\n",
    "                wrapped_lines = textwrap.wrap(match_text, width=24)\n",
    "                wrapped_caption_match = \"\\n\".join(wrapped_lines[:2])\n",
    "                ax.set_title(wrapped_caption_match, fontsize=8)\n",
    "            ax.axis('off')\n",
    "        row_count += 1\n",
    "        # when reaching max_rows_plot, save current plot and make a new plot\n",
    "        if row_count == max_rows_plot:\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(output_dir, f\"final_plot_{part}.png\"))\n",
    "            plt.close(fig)\n",
    "            part += 1\n",
    "            fig, axes = plt.subplots(rows, cols, figsize=(cols*3, rows*3))\n",
    "            row_count = 0\n",
    "            continue\n",
    "\n",
    "    if row_count < max_rows_plot:\n",
    "        for i in range(row_count, rows):\n",
    "            for j in range(cols):\n",
    "                fig.delaxes(axes[i, j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f\"final_plot_{part}.png\"))\n",
    "    plt.close(fig)\n",
    "    print(\"Final plot saved to \", os.path.join(output_dir, f\"final_plot_{part}.png\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    final_results_file = f\"data/final/{dataset_name}/final_results.json\"\n",
    "    output_dir = f\"data/final/{dataset_name}/final_plots\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    results = json.load(open(final_results_file, \"r\"))\n",
    "    show_match_results_long(dataset, results, output_dir, k=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "import os\n",
    "\n",
    "with open(\"api.json\", \"r\") as f:\n",
    "    api_key = json.load(f)[\"api_key\"]\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "def classify_caption_gpt(caption, class_name):\n",
    "    prompt = f\"\"\"\n",
    "        You are a classification system that determines if a caption is relevant to a class name.\n",
    "        Steps to determine relevance:\n",
    "        1. Extract key words from the class name and caption.\n",
    "        2. Expand the class meaning to include:\n",
    "            its synonyms, hypernyms, hyponyms, inferred words based on category;\n",
    "            Cause and Effect: e.g., \"fire\" → \"burn, heat, smoke\";\n",
    "            Functional Association: e.g., \"key\" → \"lock, door, security\";\n",
    "            Situational Association: e.g., \"beach\" → \"sand, sunshine, surfing\";\n",
    "            Common Collocations: e.g., \"eat\" → \"rice, breakfast, snacks, chopsticks\";\n",
    "        3. Matching Criteria:\n",
    "            If the caption contains the exact class name or any expanded synonym and meanings → return \"1\".\n",
    "            If the caption has no relation to the class name → return \"2\".\n",
    "\n",
    "        Class Name: {class_name}\n",
    "        Caption: {caption}\n",
    "\n",
    "        Return only \"1\" or \"2\". No explanations.\n",
    "        \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=20\n",
    "    )\n",
    "    cleaned_response = response.choices[0].message.content.strip().lower().replace('\"', '').replace(\"'\", \"\")\n",
    "    return cleaned_response\n",
    "\n",
    "# # Example usage\n",
    "# captions = [\n",
    "#     \"superme performance: the bmw m3 edition models\",\n",
    "#     \"most beautiful set of wheels on cars!!\",\n",
    "#     \"new harder bmw m3 for uk\",\n",
    "#     \"wall paper 2009 bmw m3 coupe\"\n",
    "# ]\n",
    "\n",
    "# class_name = \"bmw m3 coupe 2012\"\n",
    "\n",
    "# for caption in captions:\n",
    "#     classification = classify_caption_gpt(caption, class_name)\n",
    "#     print(f\"Caption: {caption}\")\n",
    "#     print(f\"Classification: {classification}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "output:\n",
    "    set 1: original images in dataset that has duplicates in laion\n",
    "    set 2: images in laion that has \"correct\" caption (containing the class name)\n",
    "    set 3: images in laion with \"incorrect\" caption (contains partial class name)\n",
    "    set 4: images in laion with irrelevant caption (emoji/stock photo)\n",
    "\"\"\"\n",
    "import re\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def is_fully_included(class_name, caption):\n",
    "    # Replace underscores with spaces, then extract words (ignoring punctuation)\n",
    "    # Alternatively, you can split on underscores and other non-word characters:\n",
    "    words = re.split(r'[\\W_]+', class_name)\n",
    "    words = [word for word in words if word]  # filter out empty strings\n",
    "    # Check that every word from the phrase appears as a full word in the sentence (ignore case)\n",
    "    return all(re.search(r'\\b{}\\b'.format(re.escape(word)), caption, flags=re.IGNORECASE) for word in words)\n",
    "\n",
    "def process_item(item):\n",
    "    uid, indices = item\n",
    "    _, class_name, _, _ = dataset.get_by_id(uid)\n",
    "\n",
    "    correct = 0\n",
    "    relevant = 0\n",
    "    irrelevant = 0\n",
    "\n",
    "    # all_captions.append(uid)\n",
    "    for index in indices:\n",
    "        laion_caption = laion[index][1]\n",
    "        if is_fully_included(class_name, laion_caption):\n",
    "            correct = 1\n",
    "        else:\n",
    "            response = classify_caption_gpt(laion_caption, class_name)\n",
    "            if response == \"1\":\n",
    "                relevant = 1\n",
    "            elif response == \"2\":\n",
    "                irrelevant = 1\n",
    "            else:\n",
    "                print(\"ERROR: the unexpected response:\", uid, index, laion_caption, response)\n",
    "            # print(f\"class: {class_name}, captions: {laion_caption}, result: {response}\")\n",
    "        if correct == \"1\" and relevant == \"0\" and irrelevant == \"0\":\n",
    "            only_correct = 1\n",
    "\n",
    "    return uid, correct, relevant, irrelevant\n",
    "\n",
    "final_results_file = f\"data/final/{dataset_name}/final_results.json\"\n",
    "final_results = json.load(open(final_results_file, \"r\"))\n",
    "\n",
    "print(f\"Processing results of {dataset_name}...\")\n",
    "all_captions = []\n",
    "correct_captions = []\n",
    "relevant_captions = []\n",
    "irrelevant_captions = []\n",
    "\n",
    "items = list(final_results.items())\n",
    "with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "    results = list(tqdm(executor.map(process_item, items), total=len(items)))\n",
    "\n",
    "for uid, correct, relevant, irrelevant in results:\n",
    "    all_captions.append(uid)\n",
    "    correct_captions.extend([uid] * correct)\n",
    "    relevant_captions.extend([uid] * relevant)\n",
    "    irrelevant_captions.extend([uid] * irrelevant)\n",
    "\n",
    "\n",
    "output_dir = f\"data/final/{dataset_name}/duplicate_categories\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "json.dump(all_captions, open(os.path.join(output_dir, \"all_captions.json\"), \"w\"))\n",
    "json.dump(correct_captions, open(os.path.join(output_dir, \"correct_captions.json\"), \"w\"))\n",
    "json.dump(relevant_captions, open(os.path.join(output_dir, \"relevant_captions.json\"), \"w\"))\n",
    "json.dump(irrelevant_captions, open(os.path.join(output_dir, \"irrelevant_captions.json\"), \"w\"))\n",
    "\n",
    "print(\"Done!\")\n",
    "print(len(all_captions), len(correct_captions), len(relevant_captions), len(irrelevant_captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "# dataset_name = \"food101-test\"\n",
    "output_dir = f\"data/final/{dataset_name}/duplicate_categories\"\n",
    "all_captions = json.load(open(os.path.join(output_dir, \"all_captions.json\"), \"r\"))\n",
    "correct_captions = json.load(open(os.path.join(output_dir, \"correct_captions.json\"), \"r\"))\n",
    "relevant_captions = json.load(open(os.path.join(output_dir, \"relevant_captions.json\"), \"r\"))\n",
    "irrelevant_captions = json.load(open(os.path.join(output_dir, \"irrelevant_captions.json\"), \"r\"))\n",
    "\n",
    "only_correct = []\n",
    "only_relevant = []\n",
    "only_irrelevant = []\n",
    "correct_and_relevant = []\n",
    "correct_and_irrelevant = []\n",
    "relevant_and_irrelevant = []\n",
    "mixed = []\n",
    "for uid in all_captions:\n",
    "    is_correct = uid in correct_captions\n",
    "    is_relevant = uid in relevant_captions\n",
    "    is_irrelevant = uid in irrelevant_captions\n",
    "    if is_correct and not is_relevant and not is_irrelevant:\n",
    "        only_correct.append(uid)\n",
    "    elif is_relevant and not is_correct and not is_irrelevant:\n",
    "        only_relevant.append(uid)\n",
    "    elif is_irrelevant and not is_correct and not is_relevant:\n",
    "        only_irrelevant.append(uid)\n",
    "    elif is_correct and is_relevant and not is_irrelevant:\n",
    "        correct_and_relevant.append(uid)\n",
    "    elif is_correct and is_irrelevant and not is_relevant:\n",
    "        correct_and_irrelevant.append(uid)\n",
    "    elif is_relevant and is_irrelevant and not is_correct:\n",
    "        relevant_and_irrelevant.append(uid)\n",
    "    elif is_correct and is_relevant and is_irrelevant:\n",
    "        mixed.append(uid)\n",
    "    else:\n",
    "        print(\"Error! Examine the uid again in last step!\", uid)\n",
    "if len(only_correct) + len(only_relevant) + len(only_irrelevant) + len(correct_and_relevant) + len(correct_and_irrelevant) + len(relevant_and_irrelevant) + len(mixed) != len(all_captions):\n",
    "\n",
    "    print(\"Error in classification! total number doesn't match!\")\n",
    "\n",
    "print(f\"for {dataset_name}:\")\n",
    "print(\"only_correct -\", only_correct)\n",
    "print(\"only_relevant -\", only_relevant)\n",
    "print(\"only_irrelevant -\", only_irrelevant)\n",
    "print(\"correct_and_relevant -\", correct_and_relevant)\n",
    "print(\"correct_and_irrelevant -\", correct_and_irrelevant)\n",
    "print(\"relevant_and_irrelevant -\", relevant_and_irrelevant)\n",
    "print(\"mixed -\", mixed)\n",
    "json.dump(only_correct, open(os.path.join(output_dir, \"only_correct.json\"), \"w\"))\n",
    "json.dump(only_relevant, open(os.path.join(output_dir, \"only_relevant.json\"), \"w\"))\n",
    "json.dump(only_irrelevant, open(os.path.join(output_dir, \"only_irrelevant.json\"), \"w\"))\n",
    "json.dump(correct_and_relevant, open(os.path.join(output_dir, \"correct_and_relevant.json\"), \"w\"))\n",
    "json.dump(correct_and_irrelevant, open(os.path.join(output_dir, \"correct_and_irrelevant.json\"), \"w\"))\n",
    "json.dump(relevant_and_irrelevant, open(os.path.join(output_dir, \"relevant_and_irrelevant.json\"), \"w\"))\n",
    "json.dump(mixed, open(os.path.join(output_dir, \"mixed.json\"), \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archieved Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For the class name, think of a group of words that includes:\n",
    "its synonyms, hypernyms, hyponyms, inferred words based on category;\n",
    "Cause and Effect: e.g., \"fire\" → \"burn, heat, smoke\";\n",
    "Functional Association: e.g., \"key\" → \"lock, door, security\";\n",
    "Situational Association: e.g., \"beach\" → \"sand, sunshine, surfing\";\n",
    "Common Collocations: e.g., \"eat\" → \"rice, breakfast, snacks, chopsticks\";\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "        You are a helpful AI that determines whether a caption is relevant to a given class name.\n",
    "        The class names are countries. For each class name, think about the countries historical, culture, and natural facts, people, buildings, scenary, etc\n",
    "        Categorize the caption into one of the following:\n",
    "\n",
    "        1. if the caption has overlaps with one or more facts of the class name -> return \"1\"\n",
    "        2. if the caption has nothing to do with the class -> return \"2\"\n",
    "\n",
    "        please do not return anything other than \"1\" or \"2\".\n",
    "\n",
    "        Class Name: {class_name}\n",
    "        Caption: {caption}\n",
    "\n",
    "        Response:\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "        You are a helpful AI that determines whether a caption is relevant to a given class name.\n",
    "        The class names are car models, in has informations like brand, model, year, and some specifications.\n",
    "        Categorize the caption into one of the following:\n",
    "\n",
    "        1. if the caption contains some same infomration with the class name, like model or brand -> return \"1\"\n",
    "        2. if the caption has nothing to do with the class -> return \"2\"\n",
    "\n",
    "        please do not return anything other than \"1\" or \"2\".\n",
    "\n",
    "        Class Name: {class_name}\n",
    "        Caption: {caption}\n",
    "\n",
    "        Response:\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "        You are a classification system that determines if a caption is relevant to a class name.\n",
    "        Steps to determine relevance:\n",
    "        1. Extract key words from the class name and caption, including specific breeds, pet-related terms, and general categories like \"dog\" or \"cat.\"\n",
    "        2. Expand the class meaning to include:\n",
    "            Synonyms, hypernyms, and hyponyms (e.g., \"Golden Retriever\" → \"retriever, dog, puppy\").\n",
    "            Pet-related associations (e.g., \"kitten\" → \"cat\", \"puppy\" → \"dog\").\n",
    "        3. Matching Criteria:\n",
    "            If the caption contains the exact breed name or any expanded synonym → return \"1\".\n",
    "            If the caption contains a general pet category (e.g., \"dog\" for a dog breed, \"cat\" for a cat breed) → return \"1\".\n",
    "            If the caption has no relation to the breed or pets → return \"2\".\n",
    "\n",
    "        Class Name: {class_name}\n",
    "        Caption: {caption}\n",
    "\n",
    "        Return only \"1\" or \"2\". No explanations.\n",
    "        \"\"\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
