{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset from Huggingface\n",
    "\n",
    "Optimize hf dataset for fast search and retrival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pets-train-train\n"
     ]
    }
   ],
   "source": [
    "if dataset_name == \"cifar100\":\n",
    "    hf_dataset = load_dataset(\"clip-benchmark/wds_vtab-cifar100\", split=split, streaming=False)\n",
    "elif dataset_name == \"caltech101\":\n",
    "    hf_dataset = load_dataset(\"clip-benchmark/wds_vtab-caltech101\", split=split, streaming=False)\n",
    "elif dataset_name == \"food101\":\n",
    "    hf_dataset = load_dataset(\"clip-benchmark/wds_food101\", split=split, streaming=False)\n",
    "elif dataset_name == \"cars\":\n",
    "    hf_dataset = load_dataset(\"clip-benchmark/wds_cars\", split=split, streaming=False)\n",
    "elif dataset_name == \"country211\":\n",
    "    hf_dataset = load_dataset(\"clip-benchmark/wds_country211\", split=split, streaming=False)\n",
    "elif dataset_name == \"sun397\":\n",
    "    hf_dataset = load_dataset(\"clip-benchmark/wds_sun397\", split=split, streaming=False)\n",
    "elif dataset_name == \"fer2013\":\n",
    "    hf_dataset = load_dataset(\"clip-benchmark/wds_fer2013\", split=split, streaming=False)\n",
    "elif dataset_name == \"aircraft\":\n",
    "    hf_dataset = load_dataset(\"clip-benchmark/wds_fgvc_aircraft\", split=split, streaming=False)\n",
    "elif dataset_name == \"imagenetv2\":\n",
    "    hf_dataset = load_dataset(\"clip-benchmark/wds_imagenetv2\", split=split, streaming=False)\n",
    "elif dataset_name == \"imagenet-o\":\n",
    "    hf_dataset = load_dataset(\"clip-benchmark/wds_imagenet-o\", split=split, streaming=False)\n",
    "elif dataset_name == \"pets\":\n",
    "    hf_dataset = load_dataset(\"clip-benchmark/wds_vtab-pets\", split=split, streaming=False)\n",
    "elif dataset_name == \"imagenet-a\":\n",
    "    hf_dataset = load_dataset(\"clip-benchmark/wds_imagenet-a\", split=split, streaming=False)\n",
    "elif dataset_name == \"imagenet-r\":\n",
    "    hf_dataset = load_dataset(\"clip-benchmark/wds_imagenet-r\", split=split, streaming=False)\n",
    "elif dataset_name == \"cub\":\n",
    "    hf_dataset = load_dataset(\"lxs784/cub-200-2011-clip-benchmark\", split=split, streaming=False)\n",
    "\n",
    "if \"webp\" in hf_dataset[0] and hf_dataset[0][\"webp\"] is not None:\n",
    "    image_key = \"webp\"\n",
    "elif hf_dataset[0][\"jpg\"] is not None:\n",
    "    image_key = \"jpg\"\n",
    "    \n",
    "dataset_name += \"-\" + split\n",
    "print(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "def optimize_hf_to_lightning(hf_dataset, output_dir, image_key=\"webp\", id_key=\"__key__\", label_key=\"cls\"):\n",
    "    \"\"\"\n",
    "    Iterates over the Hugging Face dataset and saves each sample to disk in a format\n",
    "    that Lightning's StreamingDataset can read. An index file (index.json) is created.\n",
    "    \n",
    "    Each sample is stored as:\n",
    "      - An image file in JPEG format\n",
    "      - A metadata entry in the index that records the file path and label\n",
    "    \n",
    "    Parameters:\n",
    "      hf_dataset: The Hugging Face dataset (can be streaming or in-memory)\n",
    "      output_dir: Directory where the optimized dataset will be stored.\n",
    "      image_key: Field name in the dataset containing image data.\n",
    "      id_key: Field name to use as a unique identifier.\n",
    "      label_key: Field name containing label or class information.\n",
    "    Returns:\n",
    "      The output directory path (which contains the data and index).\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    index = {}\n",
    "    # serializer = JPEGSerializer()  # Can be used to serialize images if desired.\n",
    "    \n",
    "    # Iterate over the dataset and write each sample.\n",
    "    for sample in hf_dataset:\n",
    "        uid = sample[id_key]\n",
    "        # Define a file path for the image.\n",
    "        image_filename = f\"{uid}.jpeg\"\n",
    "        image_path = os.path.join(output_dir, image_filename)\n",
    "        \n",
    "        # Get the image. Depending on your dataset, it might already be a PIL Image.\n",
    "        image = sample[image_key]\n",
    "        if not isinstance(image, Image.Image):\n",
    "            # If image is not a PIL image, try converting it.\n",
    "            image = Image.fromarray(image)\n",
    "            \n",
    "        if image.mode != \"RGB\":\n",
    "            image = image.convert(\"RGB\")\n",
    "        # Save the image in JPEG format.\n",
    "        image.save(image_path, format=\"JPEG\")\n",
    "        \n",
    "        # Record metadata in the index.\n",
    "        index[uid] = {\n",
    "            \"image_path\": image_filename,  # Store relative path\n",
    "            \"label\": sample[label_key],\n",
    "        }\n",
    "    \n",
    "    # Write out the index file.\n",
    "    index_path = os.path.join(output_dir, \"index.json\")\n",
    "    with open(index_path, \"w\") as f:\n",
    "        json.dump(index, f)\n",
    "    \n",
    "    return output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "from PIL import Image\n",
    "import imagehash\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class HFDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, index_file, lookup=None, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        with open(os.path.join(root_dir, index_file), \"r\") as f:\n",
    "            self.index_data = json.load(f)\n",
    "        self.lookup = lookup\n",
    "        self.samples = list(self.index_data.items())\n",
    "        self.uid_to_sample = dict(self.samples)\n",
    "        self.transform = transform \n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        uid, sample = self.samples[index]\n",
    "        image_path = os.path.join(self.root_dir, sample[\"image_path\"])\n",
    "        pil_image = Image.open(image_path).convert(\"RGB\")\n",
    "        text = self.lookup[sample[\"label\"]] if self.lookup else sample[\"label\"]\n",
    "\n",
    "        ahash = str(imagehash.average_hash(pil_image))\n",
    "        phash = str(imagehash.phash(pil_image))\n",
    "\n",
    "        return index, text, ahash, phash, uid\n",
    "\n",
    "    def get_by_id(self, uid):\n",
    "        \"\"\"\n",
    "        Retrieve a raw PIL image and metadata by its unique identifier.\n",
    "        \"\"\"\n",
    "        # if uid not in self.uid_to_sample:\n",
    "        #     raise KeyError(f\"UID: {uid} not found in dataset.\")\n",
    "        sample = self.uid_to_sample[uid]\n",
    "        image_path = os.path.join(self.root_dir, sample[\"image_path\"])\n",
    "        pil_image = Image.open(image_path).convert(\"RGB\")\n",
    "        text = self.lookup[sample[\"label\"]] if self.lookup else sample[\"label\"]\n",
    "        ahash = imagehash.average_hash(pil_image)\n",
    "        phash = str(imagehash.phash(pil_image))\n",
    "\n",
    "        return pil_image, text, ahash, phash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m optimized_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/optimized_dataset/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(optimized_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m----> 4\u001b[0m     \u001b[43moptimize_hf_to_lightning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhf_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimized_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m dataset \u001b[38;5;241m=\u001b[39m HFDataset(\n\u001b[1;32m      7\u001b[0m         index_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex.json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m         root_dir\u001b[38;5;241m=\u001b[39moptimized_dir,\n\u001b[1;32m      9\u001b[0m         lookup\u001b[38;5;241m=\u001b[39mclasses \u001b[38;5;28;01mif\u001b[39;00m classes \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;66;03m# transform = transform\u001b[39;00m\n\u001b[1;32m     11\u001b[0m         )\n\u001b[1;32m     13\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "Cell \u001b[0;32mIn[69], line 30\u001b[0m, in \u001b[0;36moptimize_hf_to_lightning\u001b[0;34m(hf_dataset, output_dir, image_key, id_key, label_key)\u001b[0m\n\u001b[1;32m     26\u001b[0m index \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# serializer = JPEGSerializer()  # Can be used to serialize images if desired.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Iterate over the dataset and write each sample.\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m hf_dataset:\n\u001b[1;32m     31\u001b[0m     uid \u001b[38;5;241m=\u001b[39m sample[id_key]\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# Define a file path for the image.\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/datasets/arrow_dataset.py:2390\u001b[0m, in \u001b[0;36mDataset.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2388\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(pa_subtable\u001b[38;5;241m.\u001b[39mnum_rows):\n\u001b[1;32m   2389\u001b[0m             pa_subtable_ex \u001b[38;5;241m=\u001b[39m pa_subtable\u001b[38;5;241m.\u001b[39mslice(i, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 2390\u001b[0m             formatted_output \u001b[38;5;241m=\u001b[39m \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2391\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpa_subtable_ex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2392\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2393\u001b[0m \u001b[43m                \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2394\u001b[0m \u001b[43m                \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_format_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2395\u001b[0m \u001b[43m                \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_output_all_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2396\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2397\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m formatted_output\n\u001b[1;32m   2398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/datasets/formatting/formatting.py:639\u001b[0m, in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    637\u001b[0m python_formatter \u001b[38;5;241m=\u001b[39m PythonFormatter(features\u001b[38;5;241m=\u001b[39mformatter\u001b[38;5;241m.\u001b[39mfeatures)\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/datasets/formatting/formatting.py:403\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable, query_type: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[RowFormat, ColumnFormat, BatchFormat]:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_column(pa_table)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/datasets/formatting/formatting.py:444\u001b[0m, in \u001b[0;36mPythonFormatter.format_row\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LazyRow(pa_table, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    443\u001b[0m row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_arrow_extractor()\u001b[38;5;241m.\u001b[39mextract_row(pa_table)\n\u001b[0;32m--> 444\u001b[0m row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_features_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m row\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/datasets/formatting/formatting.py:222\u001b[0m, in \u001b[0;36mPythonFeaturesDecoder.decode_row\u001b[0;34m(self, row)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_row\u001b[39m(\u001b[38;5;28mself\u001b[39m, row: \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m--> 222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;28;01melse\u001b[39;00m row\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/datasets/features/features.py:2045\u001b[0m, in \u001b[0;36mFeatures.decode_example\u001b[0;34m(self, example, token_per_repo_id)\u001b[0m\n\u001b[1;32m   2031\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_example\u001b[39m(\u001b[38;5;28mself\u001b[39m, example: \u001b[38;5;28mdict\u001b[39m, token_per_repo_id: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   2032\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Decode example with custom feature decoding.\u001b[39;00m\n\u001b[1;32m   2033\u001b[0m \n\u001b[1;32m   2034\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2042\u001b[0m \u001b[38;5;124;03m        `dict[str, Any]`\u001b[39;00m\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2045\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m   2046\u001b[0m         column_name: decode_nested_example(feature, value, token_per_repo_id\u001b[38;5;241m=\u001b[39mtoken_per_repo_id)\n\u001b[1;32m   2047\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_column_requires_decoding[column_name]\n\u001b[1;32m   2048\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m value\n\u001b[1;32m   2049\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m column_name, (feature, value) \u001b[38;5;129;01min\u001b[39;00m zip_dict(\n\u001b[1;32m   2050\u001b[0m             {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m example}, example\n\u001b[1;32m   2051\u001b[0m         )\n\u001b[1;32m   2052\u001b[0m     }\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/datasets/features/features.py:2046\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2031\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_example\u001b[39m(\u001b[38;5;28mself\u001b[39m, example: \u001b[38;5;28mdict\u001b[39m, token_per_repo_id: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   2032\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Decode example with custom feature decoding.\u001b[39;00m\n\u001b[1;32m   2033\u001b[0m \n\u001b[1;32m   2034\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2042\u001b[0m \u001b[38;5;124;03m        `dict[str, Any]`\u001b[39;00m\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   2045\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m-> 2046\u001b[0m         column_name: \u001b[43mdecode_nested_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2047\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_column_requires_decoding[column_name]\n\u001b[1;32m   2048\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m value\n\u001b[1;32m   2049\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m column_name, (feature, value) \u001b[38;5;129;01min\u001b[39;00m zip_dict(\n\u001b[1;32m   2050\u001b[0m             {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m example}, example\n\u001b[1;32m   2051\u001b[0m         )\n\u001b[1;32m   2052\u001b[0m     }\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/datasets/features/features.py:1404\u001b[0m, in \u001b[0;36mdecode_nested_example\u001b[0;34m(schema, obj, token_per_repo_id)\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[38;5;66;03m# Object with special decoding:\u001b[39;00m\n\u001b[1;32m   1402\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(schema, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecode_example\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(schema, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   1403\u001b[0m     \u001b[38;5;66;03m# we pass the token to read and decode files from private repositories in streaming mode\u001b[39;00m\n\u001b[0;32m-> 1404\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mschema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1405\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/datasets/features/image.py:188\u001b[0m, in \u001b[0;36mImage.decode_example\u001b[0;34m(self, value, token_per_repo_id)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     image \u001b[38;5;241m=\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mopen(BytesIO(bytes_))\n\u001b[0;32m--> 188\u001b[0m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# to avoid \"Too many open files\" errors\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image\u001b[38;5;241m.\u001b[39mgetexif()\u001b[38;5;241m.\u001b[39mget(PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mExifTags\u001b[38;5;241m.\u001b[39mBase\u001b[38;5;241m.\u001b[39mOrientation) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     image \u001b[38;5;241m=\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImageOps\u001b[38;5;241m.\u001b[39mexif_transpose(image)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/PIL/WebPImagePlugin.py:160\u001b[0m, in \u001b[0;36mWebPImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_seek(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__logical_frame)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# We need to load the image data for this frame\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m data, timestamp, duration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m timestamp\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mduration\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m duration\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/PIL/WebPImagePlugin.py:127\u001b[0m, in \u001b[0;36mWebPImageFile._get_next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_next\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;66;03m# Get next frame\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__physical_frame \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;66;03m# Check if an error occurred\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimized_dir = f\"data/optimized_dataset/{dataset_name}\"\n",
    "\n",
    "if not os.path.exists(os.path.join(optimized_dir, \"index.json\")):\n",
    "    optimize_hf_to_lightning(hf_dataset, optimized_dir, image_key=image_key)\n",
    "\n",
    "dataset = HFDataset(\n",
    "        index_file = \"index.json\",\n",
    "        root_dir=optimized_dir,\n",
    "        lookup=classes if classes else None,\n",
    "        # transform = transform\n",
    "        )\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "for _, texts, ahashes, phashes, uids in dataloader:\n",
    "    print(texts, ahashes, phashes, uids)\n",
    "    break\n",
    "sample_uid = dataset.samples[0][0]\n",
    "pil_image, text, ahash, phash = dataset.get_by_id(sample_uid)\n",
    "pil_image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the laion400m dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"load the laion400m dataset for image retrival\"\"\"\n",
    "import os\n",
    "from lightning_cloud.utils.data_connection import add_s3_connection\n",
    "from lightning.data import StreamingDataset, StreamingDataLoader\n",
    "from lightning.data.streaming.serializers import JPEGSerializer\n",
    "import torchvision.transforms.v2 as T\n",
    "from tqdm import tqdm\n",
    "import imagehash\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import concurrent\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# 1. Add the prepared dataset to your teamspace\n",
    "add_s3_connection(\"laoin-400m\")\n",
    "\n",
    "# 2. Create the streaming dataset\n",
    "class LAOINStreamingDataset(StreamingDataset):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.serializer = JPEGSerializer()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        id, image, text, _, _, _ = super().__getitem__(index)\n",
    "        \n",
    "        return Image.open(io.BytesIO(image)), text, str(id)\n",
    "\n",
    "laion = LAOINStreamingDataset(input_dir=\"/teamspace/s3_connections/laoin-400m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FILTER WITH CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "EXTRA CELL FOR RANDOM TEST\n",
    "plot original image and overlap images\n",
    "one row per plot\n",
    "\"\"\"\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import textwrap\n",
    "import json\n",
    "import clip\n",
    "import torch\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "def resize_image(image, target_size=(256, 256)):\n",
    "    return image.resize(target_size, Image.Resampling.LANCZOS)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "def show_match_results_clip(dataset, results, output_dir, final_results):\n",
    "    error_find = 0\n",
    "    error_filter = 0\n",
    "    correct_find = 0\n",
    "    correct_filter = 0\n",
    "    to_inspect = {}\n",
    "    for uid, match_indices in tqdm(results.items(), desc=f\"verifying duplicate images in {dataset_name}\"):\n",
    "        original_image, original_text, _, _= dataset.get_by_id(uid)\n",
    "        original_image_resized = resize_image(original_image)\n",
    "        orig_input = preprocess(original_image).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            orig_features = model.encode_image(orig_input)\n",
    "            orig_features /= orig_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        correct = 0\n",
    "        for j in range (len(match_indices)):\n",
    "            idx = match_indices[j]\n",
    "            match_image, match_text, _ = laion[idx]\n",
    "            match_input = preprocess(match_image).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                match_features = model.encode_image(match_input)\n",
    "                match_features /= match_features.norm(dim=-1, keepdim=True)\n",
    "            similarity = (orig_features @ match_features.T).item()\n",
    "            if similarity >= sim_threshold:\n",
    "                correct += 1\n",
    "\n",
    "        if correct > 0 and uid not in final_results:\n",
    "            error_find += 1\n",
    "            to_inspect[uid] = match_indices\n",
    "        elif correct > 0 and uid in final_results:\n",
    "            correct_find += 1\n",
    "        elif correct == 0 and uid in final_results:\n",
    "            error_filter += 1\n",
    "        else:\n",
    "            correct_filter += 1\n",
    "    print(f\"error_find: {error_find}, \\nerror_filter: {error_filter}, \\ncorrect_find: {correct_find}, \\ncorrect_filter: {correct_filter}\")\n",
    "    print(\"details in error_find: \", to_inspect, \"\\n\")\n",
    "input_dir = f\"/teamspace/studios/this_studio/data/intermediate/{dataset_name}/match_indices_4\"\n",
    "if not os.path.exists(input_dir):\n",
    "    input_dir = f\"/teamspace/studios/find-overlaps-in-laion-400m/data/intermediate/{dataset_name}/match_indices_4\"\n",
    "input_file = os.path.join(input_dir, \"combined_results.json\")\n",
    "output_dir = f\"/teamspace/studios/this_studio/data/intermediate/{dataset_name}/plots-clip\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "with open(f\"/teamspace/studios/this_studio/data/final/{dataset_name}/final_results.json\", \"r\") as f:\n",
    "    final_results = json.load(f)\n",
    "\n",
    "with open(input_file, \"r\") as f:\n",
    "    results = json.load(f)\n",
    "    show_match_results_clip(dataset, results, output_dir, final_results)\n",
    "print(\"Plotted all images: \", output_dir)\n",
    "\n",
    "correct = glob.glob(f\"/teamspace/studios/this_studio/data/intermediate/{dataset_name}/plots-clip/*.png\")\n",
    "\n",
    "\n",
    "print(\"final_results(hand picked):\", len(final_results))\n",
    "print(\"results filtered with clip:\", len(correct))\n",
    "print(\"error rate: \", round(abs(len(final_results) - len(correct))/ len(correct), 4) * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_name = cifar100-train\n",
      "images before filtering: 402 \n",
      "manual filtered results: 445 \n",
      "error_find: 0 \n",
      "error_filter: 149 \n",
      "correct_find: 0 \n",
      "correct_filter: 253\n",
      "false_positives = [] \n",
      "false_negatives = ['s0021593', 's0022113', 's0022296', 's0022578', 's0022751', 's0022774', 's0023005', 's0023826', 's0023868', 's0024160', 's0025313', 's0025717', 's0026175', 's0026226', 's0026408', 's0026544', 's0026809', 's0026856', 's0027055', 's0027271', 's0027664', 's0028238', 's0028470', 's0028537', 's0028792', 's0028811', 's0028826', 's0029131', 's0029158', 's0029510', 's0029537', 's0029562', 's0030357', 's0030950', 's0030973', 's0031182', 's0031435', 's0031625', 's0032030', 's0032440', 's0001241', 's0001244', 's0001308', 's0001659', 's0001739', 's0001893', 's0002174', 's0002504', 's0002597', 's0002747', 's0002792', 's0003175', 's0003209', 's0004417', 's0004689', 's0004763', 's0004942', 's0005122', 's0005236', 's0006484', 's0007830', 's0007886', 's0009231', 's0009563', 's0010490', 's0010810', 's0011173', 's0011196', 's0011883', 's0012280', 's0012483', 's0012901', 's0012910', 's0013120', 's0013189', 's0013368', 's0013594', 's0013640', 's0013662', 's0014021', 's0014042', 's0014126', 's0014136', 's0014156', 's0014467', 's0014522', 's0014929', 's0015133', 's0015738', 's0015938', 's0016025', 's0016295', 's0017182', 's0017399', 's0017579', 's0017772', 's0018038', 's0018314', 's0018392', 's0018463', 's0018519', 's0018881', 's0019185', 's0019527', 's0019848', 's0020085', 's0020729', 's0020769', 's0020813', 's0021229', 's0021366', 's0033299', 's0033896', 's0033914', 's0034210', 's0034288', 's0034693', 's0035041', 's0035246', 's0035760', 's0035874', 's0035882', 's0036022', 's0036972', 's0037040', 's0037110', 's0037738', 's0038181', 's0038414', 's0039402', 's0039724', 's0039757', 's0039783', 's0039940', 's0040022', 's0040191', 's0040271', 's0040400', 's0040957', 's0041431', 's0041609', 's0041635', 's0042054', 's0042698', 's0043044', 's0043261', 's0043271', 's0043682', 's0044986']\n",
      "\n",
      "dataset_name = cifar100-test\n",
      "images before filtering: 81 \n",
      "manual filtered results: 32 \n",
      "error_find: 0 \n",
      "error_filter: 32 \n",
      "correct_find: 0 \n",
      "correct_filter: 49\n",
      "false_positives = [] \n",
      "false_negatives = ['s0000136', 's0000175', 's0001054', 's0001112', 's0001158', 's0001224', 's0001758', 's0002562', 's0002595', 's0002630', 's0003181', 's0003199', 's0003545', 's0004061', 's0004437', 's0004630', 's0004633', 's0004685', 's0004747', 's0004843', 's0005484', 's0006739', 's0007384', 's0008221', 's0008460', 's0008536', 's0008633', 's0008902', 's0009338', 's0009741', 's0009742', 's0009951']\n",
      "\n",
      "dataset_name = caltech101-train\n",
      "images before filtering: 60 \n",
      "manual filtered results: 29 \n",
      "error_find: 4 \n",
      "error_filter: 1 \n",
      "correct_find: 28 \n",
      "correct_filter: 27\n",
      "false_positives = ['s0001023', 's0001262', 's0001799', 's0002531'] \n",
      "false_negatives = ['s0000302']\n",
      "\n",
      "dataset_name = caltech101-test\n",
      "images before filtering: 152 \n",
      "manual filtered results: 39 \n",
      "error_find: 8 \n",
      "error_filter: 6 \n",
      "correct_find: 33 \n",
      "correct_filter: 105\n",
      "false_positives = ['s0000108', 's0000716', 's0002165', 's0003359', 's0004898', 's0005496', 's0005888', 's0006014'] \n",
      "false_negatives = ['s0002108', 's0002482', 's0002578', 's0003155', 's0003214', 's0005345']\n",
      "\n",
      "dataset_name = pets-train\n",
      "images before filtering: 40 \n",
      "manual filtered results: 35 \n",
      "error_find: 0 \n",
      "error_filter: 3 \n",
      "correct_find: 32 \n",
      "correct_filter: 5\n",
      "false_positives = [] \n",
      "false_negatives = ['s0000997', 's0001765', 's0002224']\n",
      "\n",
      "dataset_name = pets-test\n",
      "images before filtering: 51 \n",
      "manual filtered results: 50 \n",
      "error_find: 0 \n",
      "error_filter: 3 \n",
      "correct_find: 47 \n",
      "correct_filter: 1\n",
      "false_positives = [] \n",
      "false_negatives = ['s0001332', 's0001979', 's0003416']\n",
      "\n",
      "dataset_name = imagenetv2-test\n",
      "images before filtering: 232 \n",
      "manual filtered results: 181 \n",
      "error_find: 1 \n",
      "error_filter: 22 \n",
      "correct_find: 159 \n",
      "correct_filter: 50\n",
      "false_positives = ['s0002250'] \n",
      "false_negatives = ['s0000078', 's0000354', 's0002538', 's0003656', 's0003772', 's0003874', 's0004143', 's0004767', 's0004799', 's0005602', 's0005604', 's0005648', 's0006662', 's0006761', 's0007936', 's0008001', 's0008166', 's0008247', 's0008516', 's0008600', 's0008846', 's0009537']\n",
      "\n",
      "dataset_name = imagenet-a-test\n",
      "images before filtering: 75 \n",
      "manual filtered results: 32 \n",
      "error_find: 0 \n",
      "error_filter: 6 \n",
      "correct_find: 26 \n",
      "correct_filter: 43\n",
      "false_positives = [] \n",
      "false_negatives = ['s0004766', 's0004984', 's0005449', 's0005684', 's0006332', 's0006778']\n",
      "\n",
      "dataset_name = food101-train\n",
      "images before filtering: 228 \n",
      "manual filtered results: 191 \n",
      "error_find: 0 \n",
      "error_filter: 2 \n",
      "correct_find: 189 \n",
      "correct_filter: 37\n",
      "false_positives = [] \n",
      "false_negatives = ['s0007669', 's0055625']\n",
      "\n",
      "dataset_name = food101-test\n",
      "images before filtering: 78 \n",
      "manual filtered results: 70 \n",
      "error_find: 0 \n",
      "error_filter: 0 \n",
      "correct_find: 70 \n",
      "correct_filter: 8\n",
      "false_positives = [] \n",
      "false_negatives = []\n",
      "\n",
      "dataset_name = aircraft-train\n",
      "images before filtering: 155 \n",
      "manual filtered results: 8 \n",
      "error_find: 1 \n",
      "error_filter: 4 \n",
      "correct_find: 4 \n",
      "correct_filter: 146\n",
      "false_positives = ['s0000666'] \n",
      "false_negatives = ['s0000794', 's0000933', 's0001078', 's0003288']\n",
      "\n",
      "dataset_name = aircraft-test\n",
      "images before filtering: 173 \n",
      "manual filtered results: 8 \n",
      "error_find: 5 \n",
      "error_filter: 2 \n",
      "correct_find: 6 \n",
      "correct_filter: 160\n",
      "false_positives = ['s0001730', 's0000626', 's0000631', 's0001108', 's0001123'] \n",
      "false_negatives = ['s0002205', 's0000725']\n",
      "\n",
      "dataset_name = cars-train\n",
      "images before filtering: 661 \n",
      "manual filtered results: 610 \n",
      "error_find: 3 \n",
      "error_filter: 124 \n",
      "correct_find: 486 \n",
      "correct_filter: 48\n",
      "false_positives = ['s0002945', 's0005470', 's0000936'] \n",
      "false_negatives = ['s0005929', 's0006020', 's0006027', 's0006200', 's0006293', 's0006303', 's0006320', 's0006392', 's0006452', 's0006457', 's0006774', 's0006821', 's0006828', 's0006930', 's0007050', 's0007170', 's0002489', 's0002744', 's0002851', 's0002896', 's0002971', 's0003003', 's0003012', 's0003037', 's0003038', 's0003151', 's0003198', 's0003253', 's0003488', 's0003506', 's0003512', 's0003554', 's0003561', 's0007217', 's0007290', 's0007345', 's0007353', 's0007516', 's0007520', 's0007565', 's0007578', 's0007840', 's0007887', 's0007923', 's0007981', 's0008077', 's0008104', 's0004698', 's0004793', 's0004957', 's0004959', 's0005054', 's0005058', 's0005175', 's0005329', 's0005342', 's0005358', 's0005378', 's0005567', 's0005617', 's0005625', 's0005645', 's0005665', 's0005668', 's0005719', 's0005851', 's0005854', 's0005903', 's0003816', 's0003841', 's0004019', 's0004022', 's0004041', 's0004074', 's0004131', 's0004165', 's0004225', 's0004248', 's0004258', 's0004322', 's0004343', 's0004423', 's0004479', 's0004522', 's0004669', 's0000051', 's0000076', 's0000116', 's0000125', 's0000251', 's0000285', 's0000339', 's0000498', 's0000549', 's0000576', 's0000607', 's0000646', 's0000795', 's0000820', 's0000837', 's0000943', 's0001164', 's0001332', 's0001420', 's0001430', 's0001431', 's0001485', 's0001490', 's0001495', 's0001519', 's0001525', 's0001547', 's0001616', 's0001636', 's0001661', 's0001771', 's0001867', 's0001884', 's0002028', 's0002037', 's0002065', 's0002205', 's0002230', 's0002369']\n",
      "\n",
      "dataset_name = cars-test\n",
      "images before filtering: 699 \n",
      "manual filtered results: 645 \n",
      "error_find: 5 \n",
      "error_filter: 134 \n",
      "correct_find: 511 \n",
      "correct_filter: 49\n",
      "false_positives = ['s0006177', 's0007658', 's0000302', 's0001053', 's0001189'] \n",
      "false_negatives = ['s0005918', 's0005920', 's0005973', 's0006135', 's0006193', 's0006199', 's0006211', 's0006238', 's0006260', 's0006299', 's0006302', 's0006318', 's0006445', 's0006527', 's0006554', 's0006614', 's0006653', 's0006681', 's0006743', 's0002131', 's0002148', 's0002177', 's0002196', 's0002239', 's0002295', 's0002302', 's0002322', 's0002466', 's0002516', 's0002583', 's0002693', 's0002763', 's0002776', 's0002806', 's0002829', 's0002937', 's0003000', 's0003053', 's0003122', 's0003126', 's0003262', 's0006818', 's0006886', 's0006964', 's0007087', 's0007108', 's0007121', 's0007352', 's0007475', 's0007482', 's0007514', 's0007654', 's0007823', 's0007866', 's0007867', 's0007874', 's0007877', 's0007894', 's0008031', 's0004691', 's0004863', 's0004923', 's0004929', 's0005072', 's0005304', 's0005388', 's0005448', 's0005550', 's0005706', 's0005848', 's0005849', 's0005870', 's0003388', 's0003433', 's0003493', 's0003571', 's0003711', 's0003718', 's0003761', 's0003787', 's0003899', 's0004031', 's0004070', 's0004158', 's0004184', 's0004232', 's0004281', 's0004314', 's0004456', 's0004482', 's0004493', 's0004503', 's0004530', 's0004546', 's0004577', 's0004637', 's0004648', 's0000065', 's0000151', 's0000183', 's0000287', 's0000331', 's0000346', 's0000365', 's0000447', 's0000487', 's0000508', 's0000596', 's0000659', 's0000671', 's0000765', 's0000792', 's0000795', 's0000858', 's0000907', 's0000975', 's0001015', 's0001109', 's0001176', 's0001269', 's0001465', 's0001496', 's0001548', 's0001619', 's0001622', 's0001647', 's0001697', 's0001763', 's0001774', 's0001826', 's0001859', 's0001922', 's0002069', 's0002074']\n",
      "\n",
      "dataset_name = sun397-test\n",
      "images before filtering: 3770 \n",
      "manual filtered results: 3717 \n",
      "error_find: 0 \n",
      "error_filter: 255 \n",
      "correct_find: 3462 \n",
      "correct_filter: 53\n",
      "false_positives = [] \n",
      "false_negatives = ['s0090324', 's0090517', 's0090622', 's0090661', 's0090874', 's0091453', 's0091801', 's0003140', 's0003334', 's0003660', 's0003824', 's0004984', 's0005158', 's0005192', 's0006391', 's0019648', 's0020049', 's0021660', 's0022098', 's0043402', 's0027635', 's0028201', 's0028581', 's0044441', 's0044769', 's0044959', 's0045150', 's0045251', 's0029093', 's0029257', 's0029499', 's0031894', 's0032296', 's0100550', 's0100704', 's0100787', 's0100933', 's0101050', 's0101133', 's0101370', 's0102165', 's0102175', 's0102289', 's0102650', 's0102663', 's0102748', 's0102817', 's0103134', 's0077994', 's0078808', 's0079276', 's0079568', 's0079618', 's0081222', 's0081327', 's0081382', 's0081446', 's0013141', 's0013438', 's0013498', 's0013717', 's0014414', 's0014479', 's0014501', 's0014676', 's0015941', 's0015964', 's0097661', 's0097675', 's0097867', 's0097871', 's0098862', 's0098934', 's0099182', 's0099446', 's0100275', 's0050401', 's0050499', 's0051201', 's0051450', 's0052297', 's0000234', 's0000749', 's0001165', 's0001388', 's0001408', 's0002159', 's0002483', 's0016509', 's0016641', 's0017507', 's0017510', 's0017535', 's0017539', 's0017633', 's0017776', 's0019179', 's0019436', 's0019539', 's0019544', 's0033655', 's0034398', 's0034612', 's0034614', 's0035191', 's0036520', 's0054585', 's0054692', 's0054697', 's0054698', 's0054755', 's0054784', 's0054786', 's0054811', 's0054864', 's0055159', 's0061165', 's0061359', 's0062938', 's0063240', 's0063716', 's0103485', 's0103749', 's0104318', 's0104607', 's0104820', 's0088396', 's0089159', 's0089208', 's0089406', 's0089471', 's0089555', 's0089608', 's0089925', 's0090194', 's0085808', 's0085969', 's0086393', 's0086671', 's0086975', 's0087165', 's0091886', 's0092398', 's0092449', 's0092828', 's0093405', 's0094280', 's0094288', 's0106247', 's0107507', 's0107702', 's0108404', 's0108572', 's0108676', 's0011896', 's0012067', 's0040688', 's0094944', 's0095872', 's0096934', 's0096953', 's0096993', 's0097213', 's0006652', 's0006717', 's0006922', 's0007356', 's0008422', 's0008435', 's0059900', 's0060482', 's0060498', 's0061129', 's0071178', 's0071954', 's0071956', 's0072361', 's0073150', 's0083447', 's0083626', 's0083799', 's0084168', 's0084379', 's0084813', 's0085081', 's0085738', 's0081812', 's0081919', 's0081944', 's0082076', 's0082744', 's0009765', 's0010411', 's0074776', 's0075174', 's0075185', 's0075873', 's0076597', 's0077093', 's0077635', 's0025077', 's0025177', 's0026085', 's0026132', 's0026216', 's0026619', 's0026700', 's0026771', 's0026952', 's0026996', 's0047978', 's0048193', 's0048258', 's0048314', 's0048516', 's0048849', 's0048989', 's0049039', 's0049735', 's0036932', 's0037566', 's0037776', 's0038571', 's0038782', 's0039241', 's0064626', 's0064912', 's0064934', 's0065369', 's0065548', 's0066405', 's0066495', 's0066681', 's0066875', 's0008717', 's0056104', 's0056787', 's0057056', 's0058875', 's0059550', 's0059624', 's0022626', 's0023457', 's0024501', 's0024829', 's0046684', 's0047555', 's0047721', 's0047803', 's0047864', 's0067511', 's0067691', 's0067845', 's0069646', 's0070371']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "EXTRA CELL FOR RANDOM TEST\n",
    "plot original image and overlap images\n",
    "one row per plot\n",
    "\"\"\"\n",
    "import os\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import textwrap\n",
    "import json\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "def inspect_clip_results(dataset_name):\n",
    "    input_dir = f\"/teamspace/studios/this_studio/data/intermediate/{dataset_name}/match_indices_4\"\n",
    "    if not os.path.exists(input_dir):\n",
    "        input_dir = f\"/teamspace/studios/find-overlaps-in-laion-400m/data/intermediate/{dataset_name}/match_indices_4\"\n",
    "    input_file = os.path.join(input_dir, \"combined_results.json\")\n",
    "    output_dir = f\"/teamspace/studios/this_studio/data/intermediate/{dataset_name}/plots-clip\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    with open(f\"/teamspace/studios/this_studio/data/final/{dataset_name}/final_results.json\", \"r\") as f:\n",
    "        final_results = json.load(f)\n",
    "\n",
    "    clip_results_fullpaths = glob.glob(f\"/teamspace/studios/this_studio/data/intermediate/{dataset_name}/plots-clip/*.png\")\n",
    "    clip_results = [os.path.basename(path).split('.')[0] for path in clip_results_fullpaths]\n",
    "    # print(clip_results)\n",
    "\n",
    "    error_find = 0\n",
    "    error_filter = 0\n",
    "    correct_find = 0\n",
    "    correct_filter = 0\n",
    "    error_find_list = []\n",
    "    error_filter_list = []\n",
    "\n",
    "    with open(input_file, \"r\") as f:\n",
    "        results = json.load(f)\n",
    "        \n",
    "    for uid in results.keys():\n",
    "        if uid in clip_results and uid not in final_results:\n",
    "            error_find += 1\n",
    "            error_find_list.append(uid)\n",
    "        elif uid in clip_results and uid in final_results:\n",
    "            correct_find += 1\n",
    "        elif uid not in clip_results and uid in final_results:\n",
    "            error_filter += 1\n",
    "            error_filter_list.append(uid)\n",
    "        else:\n",
    "            correct_filter += 1\n",
    "    print(\"dataset_name =\",dataset_name)\n",
    "    print(f\"images before filtering: {len(results.keys())} \\nmanual filtered results: {len(final_results.keys())} \\nerror_find: {error_find} \\nerror_filter: {error_filter} \\ncorrect_find: {correct_find} \\ncorrect_filter: {correct_filter}\")\n",
    "    print(f\"false_positives = {error_find_list} \\nfalse_negatives = {error_filter_list}\\n\")\n",
    "\n",
    "dataset_names = [\"cifar100-train\", \"cifar100-test\", \"caltech101-train\",\"caltech101-test\", \"pets-train\", \"pets-test\", \"imagenetv2-test\", \"imagenet-a-test\",\n",
    "                    \"food101-train\", \"food101-test\", \"aircraft-train\", \"aircraft-test\", \"cars-train\", \"cars-test\", \"sun397-test\"]\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    inspect_clip_results(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_name = cifar100-train\n",
      "images before filtering: 2074 \n",
      "manual filtered results: 445 \n",
      "error_find: 0 \n",
      "error_filter: 445 \n",
      "correct_find: 0 \n",
      "correct_filter: 1629\n",
      "false_positives = [] \n",
      "false_negatives = ['s0034179', 's0034210', 's0034265', 's0034288', 's0034693', 's0034749', 's0034800', 's0035041', 's0035126', 's0035246', 's0035402', 's0035403', 's0035420', 's0035472', 's0035530', 's0035708', 's0035718', 's0035760', 's0035787', 's0035874', 's0035882', 's0036013', 's0036022', 's0036045', 's0036196', 's0036436', 's0036509', 's0010818', 's0010837', 's0010873', 's0010879', 's0010987', 's0011173', 's0011196', 's0011537', 's0011674', 's0011883', 's0012233', 's0012280', 's0012364', 's0012483', 's0012606', 's0012613', 's0004513', 's0004689', 's0004763', 's0004767', 's0004808', 's0004942', 's0005122', 's0005209', 's0005236', 's0005282', 's0005373', 's0005542', 's0005960', 's0006234', 's0006378', 's0006379', 's0006471', 's0006484', 's0006569', 's0006696', 's0030245', 's0030304', 's0030357', 's0030847', 's0030950', 's0030973', 's0031182', 's0031209', 's0031435', 's0031611', 's0031625', 's0031631', 's0031641', 's0031672', 's0031701', 's0031853', 's0032030', 's0023415', 's0023506', 's0023550', 's0023567', 's0023613', 's0023725', 's0023795', 's0023826', 's0023868', 's0024160', 's0024202', 's0024464', 's0024631', 's0024730', 's0024793', 's0024842', 's0024999', 's0025313', 's0025370', 's0040713', 's0040945', 's0040957', 's0041216', 's0041431', 's0041609', 's0041635', 's0041696', 's0041784', 's0041890', 's0041975', 's0042006', 's0042054', 's0042074', 's0042087', 's0042140', 's0042357', 's0042369', 's0042698', 's0042706', 's0043014', 's0036972', 's0037040', 's0037110', 's0037337', 's0037508', 's0037633', 's0037709', 's0037738', 's0037744', 's0037808', 's0037940', 's0038006', 's0038087', 's0038181', 's0038209', 's0038249', 's0038387', 's0038414', 's0038417', 's0038566', 's0012827', 's0012858', 's0012901', 's0012910', 's0013120', 's0013140', 's0013189', 's0013368', 's0013488', 's0013590', 's0013594', 's0013598', 's0013631', 's0013640', 's0013644', 's0013662', 's0014021', 's0014042', 's0014126', 's0014136', 's0014156', 's0014462', 's0014467', 's0014522', 's0014848', 's0043044', 's0043156', 's0043201', 's0043259', 's0043261', 's0043271', 's0043682', 's0043941', 's0044092', 's0044422', 's0044546', 's0044914', 's0044968', 's0044986', 's0038701', 's0038742', 's0038766', 's0038788', 's0038883', 's0039046', 's0039151', 's0039191', 's0039242', 's0039281', 's0039402', 's0039566', 's0039724', 's0039757', 's0039783', 's0039828', 's0039940', 's0040022', 's0040029', 's0040076', 's0040140', 's0040191', 's0040256', 's0040271', 's0040287', 's0040367', 's0040400', 's0040412', 's0040414', 's0040532', 's0032187', 's0032247', 's0032409', 's0032440', 's0032545', 's0032577', 's0032640', 's0032763', 's0032850', 's0033149', 's0033259', 's0033288', 's0033299', 's0033515', 's0033870', 's0033896', 's0033914', 's0034042', 's0034142', 's0009031', 's0009231', 's0009463', 's0009563', 's0009583', 's0009978', 's0010001', 's0010032', 's0010154', 's0010350', 's0010490', 's0010501', 's0010516', 's0010548', 's0010743', 's0010808', 's0010810', 's0021366', 's0021385', 's0021409', 's0021459', 's0021541', 's0021546', 's0021592', 's0021593', 's0021755', 's0022113', 's0022131', 's0022165', 's0022296', 's0022578', 's0022751', 's0022774', 's0022780', 's0022969', 's0023005', 's0023171', 's0014929', 's0014960', 's0015018', 's0015133', 's0015307', 's0015525', 's0015738', 's0015838', 's0015868', 's0015938', 's0016010', 's0016025', 's0016070', 's0016079', 's0016108', 's0016295', 's0016412', 's0016543', 's0016703', 's0016745', 's0019000', 's0019138', 's0019185', 's0019198', 's0019395', 's0019527', 's0019631', 's0019848', 's0020070', 's0020085', 's0020141', 's0020145', 's0020261', 's0020366', 's0020404', 's0020416', 's0020729', 's0020769', 's0020805', 's0020813', 's0021229', 's0025424', 's0025468', 's0025511', 's0025717', 's0025957', 's0026040', 's0026175', 's0026182', 's0026226', 's0026237', 's0026408', 's0026524', 's0026544', 's0026646', 's0026778', 's0026809', 's0026856', 's0027055', 's0027156', 's0027199', 's0027271', 's0027283', 's0027335', 's0027361', 's0027408', 's0027441', 's0027664', 's0027669', 's0006848', 's0007353', 's0007380', 's0007521', 's0007606', 's0007609', 's0007634', 's0007721', 's0007830', 's0007857', 's0007886', 's0008024', 's0008421', 's0008534', 's0008581', 's0008590', 's0000625', 's0000668', 's0000791', 's0001241', 's0001244', 's0001308', 's0001319', 's0001339', 's0001368', 's0001519', 's0001529', 's0001582', 's0001602', 's0001659', 's0001664', 's0001739', 's0001853', 's0001893', 's0002083', 's0002088', 's0002099', 's0002174', 's0002241', 's0002300', 's0002504', 's0028033', 's0028224', 's0028238', 's0028289', 's0028308', 's0028421', 's0028460', 's0028470', 's0028537', 's0028792', 's0028811', 's0028826', 's0029131', 's0029154', 's0029158', 's0029510', 's0029520', 's0029537', 's0029562', 's0029604', 's0029639', 's0029674', 's0029733', 's0029743', 's0029771', 's0029799', 's0017182', 's0017222', 's0017299', 's0017399', 's0017579', 's0017678', 's0017771', 's0017772', 's0017773', 's0017922', 's0018038', 's0018071', 's0018201', 's0018314', 's0018387', 's0018392', 's0018462', 's0018463', 's0018519', 's0018645', 's0018766', 's0018850', 's0018881', 's0018909', 's0002592', 's0002597', 's0002645', 's0002670', 's0002705', 's0002747', 's0002792', 's0002867', 's0003038', 's0003044', 's0003057', 's0003175', 's0003176', 's0003209', 's0003427', 's0003435', 's0003672', 's0004345', 's0004380', 's0004417']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DELETE\n",
    "plot original image and overlap images\n",
    "one row per plot\n",
    "\"\"\"\n",
    "import os\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import textwrap\n",
    "import json\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "def inspect_clip_results(dataset_name):\n",
    "    input_dir = f\"/teamspace/studios/this_studio/data/intermediate/cifar100/match_indices_4\"\n",
    "    if not os.path.exists(input_dir):\n",
    "        input_dir = f\"/teamspace/studios/find-overlaps-in-laion-400m/data/intermediate/{dataset_name}/match_indices_4\"\n",
    "    input_file = os.path.join(input_dir, \"combined_results.json\")\n",
    "    output_dir = f\"/teamspace/studios/this_studio/data/intermediate/{dataset_name}/plots-clip\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    with open(f\"/teamspace/studios/this_studio/data/final/{dataset_name}/final_results.json\", \"r\") as f:\n",
    "        final_results = json.load(f)\n",
    "\n",
    "    clip_results_fullpaths = glob.glob(f\"/teamspace/studios/this_studio/data/intermediate/{dataset_name}/plots-clip/*.png\")\n",
    "    clip_results = [os.path.basename(path).split('.')[0] for path in clip_results_fullpaths]\n",
    "    # print(clip_results)\n",
    "\n",
    "    error_find = 0\n",
    "    error_filter = 0\n",
    "    correct_find = 0\n",
    "    correct_filter = 0\n",
    "    error_find_list = []\n",
    "    error_filter_list = []\n",
    "\n",
    "    with open(input_file, \"r\") as f:\n",
    "        results = json.load(f)\n",
    "        \n",
    "    for uid in results.keys():\n",
    "        if uid in clip_results and uid not in final_results:\n",
    "            error_find += 1\n",
    "            error_find_list.append(uid)\n",
    "        elif uid in clip_results and uid in final_results:\n",
    "            correct_find += 1\n",
    "        elif uid not in clip_results and uid in final_results:\n",
    "            error_filter += 1\n",
    "            error_filter_list.append(uid)\n",
    "        else:\n",
    "            correct_filter += 1\n",
    "    print(\"dataset_name =\",dataset_name)\n",
    "    print(f\"images before filtering: {len(results.keys())} \\nmanual filtered results: {len(final_results.keys())} \\nerror_find: {error_find} \\nerror_filter: {error_filter} \\ncorrect_find: {correct_find} \\ncorrect_filter: {correct_filter}\")\n",
    "    print(f\"false_positives = {error_find_list} \\nfalse_negatives = {error_filter_list}\\n\")\n",
    "\n",
    "# dataset_names = [\"cifar100-train\", \"cifar100-test\", \"caltech101-train\",\"caltech101-test\", \"pets-train\", \"pets-test\", \"imagenetv2-test\", \"imagenet-a-test\",\n",
    "#                     \"food101-train\", \"food101-test\", \"aircraft-train\", \"aircraft-test\", \"cars-train\", \"cars-test\", \"sun397-test\"]\n",
    "\n",
    "# for dataset_name in dataset_names:\n",
    "    # inspect_clip_results(dataset_name)\n",
    "dataset_name = \"cifar100-train\"\n",
    "inspect_clip_results(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "EXTRA CELL FOR RANDOM TEST\n",
    "plot original image and overlap images\n",
    "one row per plot\n",
    "\"\"\"\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import textwrap\n",
    "import json\n",
    "import clip\n",
    "import torch\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "def resize_image(image, target_size=(256, 256)):\n",
    "    return image.resize(target_size, Image.Resampling.LANCZOS)\n",
    "    \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "def single_plot(dataset_name, uid, match_indices, output_dir, k=5):\n",
    "    cols = k + 2\n",
    "\n",
    "    fig, axes = plt.subplots(1, cols, figsize=(cols * 3, 3))\n",
    "    axes[0].text(0.5, 0.5, uid, fontsize=24, ha='center', va='center')\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    original_image, original_text, _, _= dataset.get_by_id(uid)\n",
    "    original_image_resized = resize_image(original_image)\n",
    "    axes[1].imshow(original_image_resized)\n",
    "    wrapped_caption = \"\\n\".join(textwrap.wrap(original_text, width=24))\n",
    "    axes[1].set_title(wrapped_caption)\n",
    "    axes[1].axis('off')\n",
    "    orig_input = preprocess(original_image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        orig_features = model.encode_image(orig_input)\n",
    "        orig_features /= orig_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    for j in range (k):\n",
    "        ax = axes[j + 2]\n",
    "        if j >= len(match_indices):\n",
    "            ax.imshow(np.ones((1, 1, 3)))\n",
    "            ax.axis('off')\n",
    "        else:\n",
    "            idx = match_indices[j]\n",
    "            match_image, match_text, _ = laion[idx]\n",
    "            match_input = preprocess(match_image).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                match_features = model.encode_image(match_input)\n",
    "                match_features /= match_features.norm(dim=-1, keepdim=True)\n",
    "            similarity = (orig_features @ match_features.T).item()\n",
    "\n",
    "            ax.imshow(match_image)\n",
    "            caption = f\"sim: {similarity:.2f}\\n\" + match_text\n",
    "            wrapped_lines = textwrap.wrap(caption, width=24)\n",
    "            wrapped_caption_match = \"\\n\".join(wrapped_lines[:2])\n",
    "            ax.set_title(wrapped_caption_match, fontsize=8)\n",
    "            ax.axis('off')\n",
    "            \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f\"{dataset_name}-{uid}.png\"))\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_name = caltech101-train\n",
    "# false_positives = ['s0001023', 's0001262', 's0001799', 's0002531'] \n",
    "# false_negatives = ['s0000302']\n",
    "\n",
    "# dataset_name = \"caltech101\"\n",
    "# split = \"test\"\n",
    "# false_positives = ['s0000108', 's0000716', 's0002165', 's0003359', 's0004898', 's0005496', 's0005888', 's0006014'] \n",
    "# false_negatives = ['s0002108', 's0002482', 's0002578', 's0003155', 's0003214', 's0005345']\n",
    "\n",
    "# dataset_name = \"pets-train\"\n",
    "# false_positives = [] \n",
    "# false_negatives = ['s0000997', 's0001765', 's0002224']\n",
    "\n",
    "# dataset_name = \"pets-test\"\n",
    "# false_positives = [] \n",
    "# false_negatives = ['s0001332', 's0001979', 's0003416']\n",
    "\n",
    "# dataset_name = \"imagenetv2-test\"\n",
    "# false_positives = ['s0002250'] \n",
    "# false_negatives = ['s0000078', 's0000354', 's0002538', 's0003656', 's0003772', 's0003874', 's0004143', 's0004767', 's0004799', 's0005602', 's0005604', 's0005648', 's0006662', 's0006761', 's0007936', 's0008001', 's0008166', 's0008247', 's0008516', 's0008600', 's0008846', 's0009537']\n",
    "\n",
    "# dataset_name = \"imagenet-a-test\"\n",
    "# false_positives = [] \n",
    "# false_negatives = ['s0004766', 's0004984', 's0005449', 's0005684', 's0006332', 's0006778']\n",
    "\n",
    "# dataset_name = \"food101-train\"\n",
    "# false_positives = [] \n",
    "# false_negatives = ['s0007669', 's0055625']\n",
    "\n",
    "# dataset_name = \"food101-test\"\n",
    "# false_positives = [] \n",
    "# false_negatives = []\n",
    "\n",
    "# dataset_name = \"aircraft-train\"\n",
    "# false_positives = ['s0000666'] \n",
    "# false_negatives = ['s0000794', 's0000933', 's0001078', 's0003288']\n",
    "\n",
    "# dataset_name = \"aircraft-test\"\n",
    "# false_positives = ['s0001730', 's0000626', 's0000631', 's0001108', 's0001123'] \n",
    "# false_negatives = ['s0002205', 's0000725']\n",
    "\n",
    "# dataset_name = \"cars-train\"\n",
    "# false_positives = ['s0002945', 's0005470', 's0000936'] \n",
    "# false_negatives = ['s0005929', 's0006020', 's0006027', 's0006200', 's0006293', 's0006303', 's0006320', 's0006392', 's0006452', 's0006457', 's0006774', 's0006821', 's0006828', 's0006930', 's0007050', 's0007170', 's0002489', 's0002744', 's0002851', 's0002896', 's0002971', 's0003003', 's0003012', 's0003037', 's0003038', 's0003151', 's0003198', 's0003253', 's0003488', 's0003506', 's0003512', 's0003554', 's0003561', 's0007217', 's0007290', 's0007345', 's0007353', 's0007516', 's0007520', 's0007565', 's0007578', 's0007840', 's0007887', 's0007923', 's0007981', 's0008077', 's0008104', 's0004698', 's0004793', 's0004957', 's0004959', 's0005054', 's0005058', 's0005175', 's0005329', 's0005342', 's0005358', 's0005378', 's0005567', 's0005617', 's0005625', 's0005645', 's0005665', 's0005668', 's0005719', 's0005851', 's0005854', 's0005903', 's0003816', 's0003841', 's0004019', 's0004022', 's0004041', 's0004074', 's0004131', 's0004165', 's0004225', 's0004248', 's0004258', 's0004322', 's0004343', 's0004423', 's0004479', 's0004522', 's0004669', 's0000051', 's0000076', 's0000116', 's0000125', 's0000251', 's0000285', 's0000339', 's0000498', 's0000549', 's0000576', 's0000607', 's0000646', 's0000795', 's0000820', 's0000837', 's0000943', 's0001164', 's0001332', 's0001420', 's0001430', 's0001431', 's0001485', 's0001490', 's0001495', 's0001519', 's0001525', 's0001547', 's0001616', 's0001636', 's0001661', 's0001771', 's0001867', 's0001884', 's0002028', 's0002037', 's0002065', 's0002205', 's0002230', 's0002369']\n",
    "\n",
    "# dataset_name = \"cars-test\"\n",
    "# false_positives = ['s0006177', 's0007658', 's0000302', 's0001053', 's0001189'] \n",
    "# false_negatives = ['s0005918', 's0005920', 's0005973', 's0006135', 's0006193', 's0006199', 's0006211', 's0006238', 's0006260', 's0006299', 's0006302', 's0006318', 's0006445', 's0006527', 's0006554', 's0006614', 's0006653', 's0006681', 's0006743', 's0002131', 's0002148', 's0002177', 's0002196', 's0002239', 's0002295', 's0002302', 's0002322', 's0002466', 's0002516', 's0002583', 's0002693', 's0002763', 's0002776', 's0002806', 's0002829', 's0002937', 's0003000', 's0003053', 's0003122', 's0003126', 's0003262', 's0006818', 's0006886', 's0006964', 's0007087', 's0007108', 's0007121', 's0007352', 's0007475', 's0007482', 's0007514', 's0007654', 's0007823', 's0007866', 's0007867', 's0007874', 's0007877', 's0007894', 's0008031', 's0004691', 's0004863', 's0004923', 's0004929', 's0005072', 's0005304', 's0005388', 's0005448', 's0005550', 's0005706', 's0005848', 's0005849', 's0005870', 's0003388', 's0003433', 's0003493', 's0003571', 's0003711', 's0003718', 's0003761', 's0003787', 's0003899', 's0004031', 's0004070', 's0004158', 's0004184', 's0004232', 's0004281', 's0004314', 's0004456', 's0004482', 's0004493', 's0004503', 's0004530', 's0004546', 's0004577', 's0004637', 's0004648', 's0000065', 's0000151', 's0000183', 's0000287', 's0000331', 's0000346', 's0000365', 's0000447', 's0000487', 's0000508', 's0000596', 's0000659', 's0000671', 's0000765', 's0000792', 's0000795', 's0000858', 's0000907', 's0000975', 's0001015', 's0001109', 's0001176', 's0001269', 's0001465', 's0001496', 's0001548', 's0001619', 's0001622', 's0001647', 's0001697', 's0001763', 's0001774', 's0001826', 's0001859', 's0001922', 's0002069', 's0002074']\n",
    "\n",
    "dataset_name = \"sun397-test\"\n",
    "false_positives = [] \n",
    "false_negatives = ['s0090324', 's0090517', 's0090622', 's0090661', 's0090874', 's0091453', 's0091801', 's0003140', 's0003334', 's0003660', 's0003824', 's0004984', 's0005158', 's0005192', 's0006391', 's0019648', 's0020049', 's0021660', 's0022098', 's0043402', 's0027635', 's0028201', 's0028581', 's0044441', 's0044769', 's0044959', 's0045150', 's0045251', 's0029093', 's0029257', 's0029499', 's0031894', 's0032296', 's0100550', 's0100704', 's0100787', 's0100933', 's0101050', 's0101133', 's0101370', 's0102165', 's0102175', 's0102289', 's0102650', 's0102663', 's0102748', 's0102817', 's0103134', 's0077994', 's0078808', 's0079276', 's0079568', 's0079618', 's0081222', 's0081327', 's0081382', 's0081446', 's0013141', 's0013438', 's0013498', 's0013717', 's0014414', 's0014479', 's0014501', 's0014676', 's0015941', 's0015964', 's0097661', 's0097675', 's0097867', 's0097871', 's0098862', 's0098934', 's0099182', 's0099446', 's0100275', 's0050401', 's0050499', 's0051201', 's0051450', 's0052297', 's0000234', 's0000749', 's0001165', 's0001388', 's0001408', 's0002159', 's0002483', 's0016509', 's0016641', 's0017507', 's0017510', 's0017535', 's0017539', 's0017633', 's0017776', 's0019179', 's0019436', 's0019539', 's0019544', 's0033655', 's0034398', 's0034612', 's0034614', 's0035191', 's0036520', 's0054585', 's0054692', 's0054697', 's0054698', 's0054755', 's0054784', 's0054786', 's0054811', 's0054864', 's0055159', 's0061165', 's0061359', 's0062938', 's0063240', 's0063716', 's0103485', 's0103749', 's0104318', 's0104607', 's0104820', 's0088396', 's0089159', 's0089208', 's0089406', 's0089471', 's0089555', 's0089608', 's0089925', 's0090194', 's0085808', 's0085969', 's0086393', 's0086671', 's0086975', 's0087165', 's0091886', 's0092398', 's0092449', 's0092828', 's0093405', 's0094280', 's0094288', 's0106247', 's0107507', 's0107702', 's0108404', 's0108572', 's0108676', 's0011896', 's0012067', 's0040688', 's0094944', 's0095872', 's0096934', 's0096953', 's0096993', 's0097213', 's0006652', 's0006717', 's0006922', 's0007356', 's0008422', 's0008435', 's0059900', 's0060482', 's0060498', 's0061129', 's0071178', 's0071954', 's0071956', 's0072361', 's0073150', 's0083447', 's0083626', 's0083799', 's0084168', 's0084379', 's0084813', 's0085081', 's0085738', 's0081812', 's0081919', 's0081944', 's0082076', 's0082744', 's0009765', 's0010411', 's0074776', 's0075174', 's0075185', 's0075873', 's0076597', 's0077093', 's0077635', 's0025077', 's0025177', 's0026085', 's0026132', 's0026216', 's0026619', 's0026700', 's0026771', 's0026952', 's0026996', 's0047978', 's0048193', 's0048258', 's0048314', 's0048516', 's0048849', 's0048989', 's0049039', 's0049735', 's0036932', 's0037566', 's0037776', 's0038571', 's0038782', 's0039241', 's0064626', 's0064912', 's0064934', 's0065369', 's0065548', 's0066405', 's0066495', 's0066681', 's0066875', 's0008717', 's0056104', 's0056787', 's0057056', 's0058875', 's0059550', 's0059624', 's0022626', 's0023457', 's0024501', 's0024829', 's0046684', 's0047555', 's0047721', 's0047803', 's0047864', 's0067511', 's0067691', 's0067845', 's0069646', 's0070371']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot fp and fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sun397 test\n",
      "397\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from torchvision import transforms\n",
    "import json\n",
    "\"\"\"\n",
    "Parameters:\n",
    "\"\"\"\n",
    "if dataset_name.split('-')[-1] == \"train\":\n",
    "    dataset_name = dataset_name[:-6]\n",
    "    split = \"train\"\n",
    "else:\n",
    "    dataset_name = dataset_name[:-5]\n",
    "    split = \"test\"\n",
    "print(dataset_name, split)\n",
    "\n",
    "# dataset_name = \"caltech101\"\n",
    "# split = \"train\"\n",
    "# sim_threshold = 0.85\n",
    "\n",
    "k = 5\n",
    "\n",
    "classes = json.load(open(f\"data/classes_{dataset_name}.json\", \"r\"))\n",
    "print(len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e143cad0b7a84983a997a8375bd4c3d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "363aeb7c3c1f49d3806e20321885164c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sun397-test\n"
     ]
    }
   ],
   "source": [
    "if dataset_name == \"cifar100\":\n",
    "    hf_dataset = load_dataset(\"clip-benchmark/wds_vtab-cifar100\", split=split, streaming=False)\n",
    "elif dataset_name == \"caltech101\":\n",
    "    hf_dataset = load_dataset(\"clip-benchmark/wds_vtab-caltech101\", split=split, streaming=False)\n",
    "elif dataset_name == \"food101\":\n",
    "    hf_dataset = load_dataset(\"clip-benchmark/wds_food101\", split=split, streaming=False)\n",
    "elif dataset_name == \"cars\":\n",
    "    hf_dataset = load_dataset(\"clip-benchmark/wds_cars\", split=split, streaming=False)\n",
    "elif dataset_name == \"country211\":\n",
    "    hf_dataset = load_dataset(\"clip-benchmark/wds_country211\", split=split, streaming=False)\n",
    "elif dataset_name == \"sun397\":\n",
    "    hf_dataset = load_dataset(\"clip-benchmark/wds_sun397\", split=split, streaming=False)\n",
    "elif dataset_name == \"fer2013\":\n",
    "    hf_dataset = load_dataset(\"clip-benchmark/wds_fer2013\", split=split, streaming=False)\n",
    "elif dataset_name == \"aircraft\":\n",
    "    hf_dataset = load_dataset(\"clip-benchmark/wds_fgvc_aircraft\", split=split, streaming=False)\n",
    "elif dataset_name == \"imagenetv2\":\n",
    "    hf_dataset = load_dataset(\"clip-benchmark/wds_imagenetv2\", split=split, streaming=False)\n",
    "elif dataset_name == \"imagenet-o\":\n",
    "    hf_dataset = load_dataset(\"clip-benchmark/wds_imagenet-o\", split=split, streaming=False)\n",
    "elif dataset_name == \"pets\":\n",
    "    hf_dataset = load_dataset(\"clip-benchmark/wds_vtab-pets\", split=split, streaming=False)\n",
    "elif dataset_name == \"imagenet-a\":\n",
    "    hf_dataset = load_dataset(\"clip-benchmark/wds_imagenet-a\", split=split, streaming=False)\n",
    "elif dataset_name == \"imagenet-r\":\n",
    "    hf_dataset = load_dataset(\"clip-benchmark/wds_imagenet-r\", split=split, streaming=False)\n",
    "\n",
    "if \"webp\" in hf_dataset[0] and hf_dataset[0][\"webp\"] is not None:\n",
    "    image_key = \"webp\"\n",
    "elif hf_dataset[0][\"jpg\"] is not None:\n",
    "    image_key = \"jpg\"\n",
    "    \n",
    "dataset_name += \"-\" + split\n",
    "print(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "from PIL import Image\n",
    "import imagehash\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class HFDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, index_file, lookup=None, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        with open(os.path.join(root_dir, index_file), \"r\") as f:\n",
    "            self.index_data = json.load(f)\n",
    "        self.lookup = lookup\n",
    "        self.samples = list(self.index_data.items())\n",
    "        self.uid_to_sample = dict(self.samples)\n",
    "        self.transform = transform \n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        uid, sample = self.samples[index]\n",
    "        image_path = os.path.join(self.root_dir, sample[\"image_path\"])\n",
    "        pil_image = Image.open(image_path).convert(\"RGB\")\n",
    "        text = self.lookup[sample[\"label\"]] if self.lookup else sample[\"label\"]\n",
    "\n",
    "        ahash = str(imagehash.average_hash(pil_image))\n",
    "        phash = str(imagehash.phash(pil_image))\n",
    "\n",
    "        return index, text, ahash, phash, uid\n",
    "\n",
    "    def get_by_id(self, uid):\n",
    "        \"\"\"\n",
    "        Retrieve a raw PIL image and metadata by its unique identifier.\n",
    "        \"\"\"\n",
    "        # if uid not in self.uid_to_sample:\n",
    "        #     raise KeyError(f\"UID: {uid} not found in dataset.\")\n",
    "        sample = self.uid_to_sample[uid]\n",
    "        image_path = os.path.join(self.root_dir, sample[\"image_path\"])\n",
    "        pil_image = Image.open(image_path).convert(\"RGB\")\n",
    "        text = self.lookup[sample[\"label\"]] if self.lookup else sample[\"label\"]\n",
    "        ahash = imagehash.average_hash(pil_image)\n",
    "        phash = str(imagehash.phash(pil_image))\n",
    "\n",
    "        return pil_image, text, ahash, phash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_dir = f\"data/optimized_dataset/{dataset_name}\"\n",
    "\n",
    "if not os.path.exists(os.path.join(optimized_dir, \"index.json\")):\n",
    "    optimize_hf_to_lightning(hf_dataset, optimized_dir, image_key=image_key)\n",
    "\n",
    "dataset = HFDataset(\n",
    "        index_file = \"index.json\",\n",
    "        root_dir=optimized_dir,\n",
    "        lookup=classes if classes else None,\n",
    "        # transform = transform\n",
    "        )\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "# for _, texts, ahashes, phashes, uids in dataloader:\n",
    "#     print(texts, ahashes, phashes, uids)\n",
    "#     break\n",
    "# sample_uid = dataset.samples[0][0]\n",
    "# pil_image, text, ahash, phash = dataset.get_by_id(sample_uid)\n",
    "# pil_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done moving false positives(error find)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/255 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 255/255 [15:38<00:00,  3.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done moving false negatives(error filter)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# move images\n",
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "fp_path = \"clip_mistakes/false_positives\"\n",
    "fn_path = \"clip_mistakes/false_negatives\"\n",
    "os.makedirs(fp_path, exist_ok=True)\n",
    "os.makedirs(fn_path, exist_ok=True)\n",
    "\n",
    "source_dir_fp = f\"/teamspace/studios/this_studio/data/intermediate/{dataset_name}/plots-clip\"\n",
    "source_dir_fn = f\"data/final/{dataset_name}/final_results.json\"\n",
    "\n",
    "for uid in false_positives:\n",
    "    image_path = os.path.join(source_dir_fp, f\"{uid}.png\")\n",
    "    new_path = os.path.join(fp_path, f\"{dataset_name}-{uid}.png\")\n",
    "    shutil.copy(image_path, new_path)\n",
    "print(\"done moving false positives(error find)\")\n",
    "source_json = json.load(open(source_dir_fn, \"r\"))\n",
    "for uid in tqdm(false_negatives, total=len(false_negatives)):\n",
    "    match_indices = source_json[uid]\n",
    "    single_plot(dataset_name, uid, match_indices, fn_path, k=5)\n",
    "print(\"done moving false negatives(error filter)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# EXTRA CELL FOR RANDOM TEST\n",
    "# plot original image and overlap images\n",
    "# one row per plot\n",
    "# \"\"\"\n",
    "# from PIL import Image, UnidentifiedImageError\n",
    "# from io import BytesIO\n",
    "# import matplotlib.pyplot as plt\n",
    "# from tqdm import tqdm\n",
    "# import pandas as pd\n",
    "# import textwrap\n",
    "# import json\n",
    "# import glob\n",
    "\n",
    "# def resize_image(image, target_size=(256, 256)):\n",
    "#     return image.resize(target_size, Image.Resampling.LANCZOS)\n",
    "\n",
    "# def show_match_results_single_wahash(dataset, results, output_dir, k=5):\n",
    "\n",
    "#     cols = k + 2\n",
    "#     for uid, match_indices in tqdm(results.items(), desc=f\"plotting duplicate images for {dataset_name}\"):\n",
    "#         fig, axes = plt.subplots(1, cols, figsize=(cols * 3, 3))\n",
    "#         axes[0].text(0.5, 0.5, uid, fontsize=24, ha='center', va='center')\n",
    "#         axes[0].axis(\"off\")\n",
    "\n",
    "#         original_image, original_text, ahash, phash= dataset.get_by_id(uid)\n",
    "#         original_image_resized = resize_image(original_image)\n",
    "#         axes[1].imshow(original_image_resized)\n",
    "#         wrapped_caption = \"\\n\".join(textwrap.wrap(original_text, width=24))\n",
    "#         axes[1].set_title(wrapped_caption)\n",
    "#         axes[1].axis('off')\n",
    "\n",
    "#         correct = 0\n",
    "#         for j in range (k):\n",
    "#             ax = axes[j + 2]\n",
    "#             if j >= len(match_indices):\n",
    "#                 ax.imshow(np.ones((1, 1, 3)))\n",
    "#             else:\n",
    "#                 idx = match_indices[j]\n",
    "#                 match_image, match_text, _ = laion[idx]\n",
    "#                 laion_phash = imagehash.phash(match_image)\n",
    "#                 p_dist = abs(imagehash.hex_to_hash(phash) - laion_phash)\n",
    "#                 laion_ahash = imagehash.average_hash(match_image)\n",
    "#                 a_dist = abs(ahash - laion_ahash)\n",
    "#                 if a_dist <= 4:\n",
    "#                     ax.imshow(match_image)\n",
    "#                     warapped_lines = \"a_dist: \" + str(a_dist) + \", p_dist: \" + str(a_dist)\n",
    "#                     wrapped_caption_match = \"\\n\".join(wrapped_lines[:2])\n",
    "#                     ax.set_title(wrapped_caption_match, fontsize=8)\n",
    "#                     correct += 1\n",
    "#                 else:\n",
    "#                     ax.imshow(np.ones((1, 1, 3)))\n",
    "#             ax.axis('off')\n",
    "#         if correct > 0:\n",
    "#             plt.tight_layout()\n",
    "#             plt.savefig(os.path.join(output_dir, f\"{uid}.png\"))\n",
    "#         plt.close(fig)\n",
    "\n",
    "# # find-overlaps-in-laion-400m\n",
    "# input_dir = f\"/teamspace/studios/this_studio/data/intermediate/{dataset_name}/match_indices_{threshold}\"\n",
    "# input_file = os.path.join(input_dir, \"combined_results.json\")\n",
    "# output_dir = f\"/teamspace/studios/this_studio/data/intermediate/{dataset_name}/plots-ahash\"\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# with open(input_file, \"r\") as f:\n",
    "#     results = json.load(f)\n",
    "#     show_match_results_single_wahash(dataset, results, output_dir, k)\n",
    "# print(\"Plotted all images: \", output_dir)\n",
    "\n",
    "# correct = glob.glob(f\"/teamspace/studios/this_studio/data/intermediate/{dataset_name}/plots-ahash/*.png\")\n",
    "\n",
    "# with open(f\"/teamspace/studios/this_studio/data/final/{dataset_name}/final_results.json\", \"r\") as f:\n",
    "#     final_results = json.load(f)\n",
    "\n",
    "# print(\"final_results(hand picked):\", len(final_results))\n",
    "# print(\"results filtered with average hash:\", len(correct))\n",
    "# print(\"error rate: \", round(abs(len(final_results) - len(correct))/ len(correct), 4) * 100, \"%\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
